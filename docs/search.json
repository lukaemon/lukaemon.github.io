[
  {
    "objectID": "posts/2023/humor-ai/index.html",
    "href": "posts/2023/humor-ai/index.html",
    "title": "Humor in AI",
    "section": "",
    "text": "Humor is preconditioned on the ability to see the bright side of something. One has to be able to see many sides, consciously choose the funny, optimistic interpretation and express in a way that resonates with target audiences. It shows both raw intelligence and wisdom.\nTo detect humor and be humorous, grounding is necessary. Grounding to me is weaving modalities. Just a fancy way of saying having sampled variety of experiences of certain things or events.\nFor example, to know what it really means about apple, one could write “apple”, read about it, draw, hold, throw, smell, eat, plant, cook, even share it with others. Without grounding, one can’t have acute and diversified perspectives on a thing or an event. It would be very hard to see ironic yet optimistic interpretation, be it human or AI.\nGrounding AI to full set of human experience is aligning computational humor to humans’. They may be able to see a kind of digital humor that is a bridge too far for us. Literally why Samantha in Her leaving Theodore1.1 youtube\nHumor is the ultimate Turing test. I see this as the source of Yann’s recent debate with others2. It’s the most difficult test to pass. It’s the most difficult test to create.2 tweet\n\n\n\n\n\n\nCitationBibTeX citation:@online{shen2023,\n  author = {Lucas Shen},\n  title = {Humor in {AI}},\n  date = {2023-02-05},\n  url = {https://lukaemon.github.io/posts/2023/humor-ai},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLucas Shen. 2023. “Humor in AI.” February 5, 2023. https://lukaemon.github.io/posts/2023/humor-ai."
  },
  {
    "objectID": "posts/2023/t5-finetuning-hello-world/samsum.html",
    "href": "posts/2023/t5-finetuning-hello-world/samsum.html",
    "title": "Hello, world! Huggingface T5 finetuning",
    "section": "",
    "text": "A learning note from reproducing this amazing post by Philipp Schmid."
  },
  {
    "objectID": "posts/2023/t5-finetuning-hello-world/samsum.html#load-dataset",
    "href": "posts/2023/t5-finetuning-hello-world/samsum.html#load-dataset",
    "title": "Hello, world! Huggingface T5 finetuning",
    "section": "Load dataset",
    "text": "Load dataset\nsamsum is a conversation dataset. The goal is to summarize a conversation. Dataset is available on Huggingface.\n\nds = load_dataset(dataset_name)\nds\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 818\n    })\n})\n\n\n\nexample = ds[\"train\"][0]\nexample\n\n{'id': '13818513',\n 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}"
  },
  {
    "objectID": "posts/2023/t5-finetuning-hello-world/samsum.html#max_length-analysis",
    "href": "posts/2023/t5-finetuning-hello-world/samsum.html#max_length-analysis",
    "title": "Hello, world! Huggingface T5 finetuning",
    "section": "max_length analysis",
    "text": "max_length analysis\nInvestigate truncation and padding to get statistics on dialogue and summary token length.\nOutlier long input may cause out of memory error during training.\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\nmodel.parallelize()\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n\ntk_dialogue = tokenizer(ds[\"train\"][\"dialogue\"])[\"input_ids\"]\ntk_summary = tokenizer(ds[\"train\"][\"summary\"])[\"input_ids\"]\npd.set_option('display.float_format', lambda x: '%.1f' % x)\n\ndf = pd.DataFrame(\n    {\"dialogue\": [len(d) for d in tk_dialogue], \"summary\": [len(s) for s in tk_summary]}\n)\nprint(df.describe())\n\n       dialogue  summary\ncount   14732.0  14732.0\nmean      149.0     28.9\nstd       110.7     15.1\nmin         1.0      2.0\n25%        66.0     17.0\n50%       120.0     26.0\n75%       202.0     37.0\nmax      1153.0     94.0\n\n\nMy first hunch is I shouldn’t truncate the input. Just need to pad to the longest of the batch. The setting would be tokenizer(batch_sentences, padding=True).\nHowever, it seems that truncation is inevitable in production. You need to find a balance and curb the long input outlier.\nFor this dataset, 1153 max is not too crazy.\n\nPadding experiments\nLet’s experiment with different padding strategy and how it affects the following batching and training.\nFirst, do it without truncation:\n\ntk_dialogue = tokenizer(ds[\"train\"][\"dialogue\"], padding=True)[\"input_ids\"]\ntk_summary = tokenizer(ds[\"train\"][\"summary\"], padding=True)[\"input_ids\"]\npd.set_option('display.float_format', lambda x: '%.1f' % x)\n\ndf = pd.DataFrame(\n    {\"dialogue\": [len(d) for d in tk_dialogue], \"summary\": [len(s) for s in tk_summary]}\n)\nprint(df.describe())\n\n       dialogue  summary\ncount   14732.0  14732.0\nmean     1153.0     94.0\nstd         0.0      0.0\nmin      1153.0     94.0\n25%      1153.0     94.0\n50%      1153.0     94.0\n75%      1153.0     94.0\nmax      1153.0     94.0\n\n\nExpected result. This is literally treating the whole training corpus as one full batch. All sequences are pad to the max length, 1153.\nTry this idea with batch_size = 8 in dataloader.\n\nfrom torch.utils.data import DataLoader\n\ncollator = DataCollatorForSeq2Seq(tokenizer, padding=True)\ndl = DataLoader(ds['train'].with_transform(lambda x: tokenizer(x['dialogue'])), batch_size=8, collate_fn=collator)\n\n\ntk_batched = np.array([batch['input_ids'].shape[-1] for batch in dl])\n\nprint(len(tk_batched), len(dl))\nprint(len(np.unique(tk_batched)))\n\nnp.unique(tk_batched).max(), np.unique(tk_batched).mean(), np.unique(tk_batched).min()b\n\n1842 1842\n482\n\n\n(1153, 389.02904564315355, 92)\n\n\n1842 batches, with 482 unique length. This is fine for pytorch but would be brutal for jax jit since every change of input shape would trigger jit recompilation.\n\nIf training on TPU, it is recommended to pad all examples of the dataset to the same length or make use of pad_to_multiple_of to have a small number of predefined bucket sizes to fit all examples in. Dynamically padding batches to the longest example is not recommended on TPU as it triggers a recompilation for every batch shape that is encountered during training thus significantly slowing down the training. only padding up to the longest example in a batch) leads to very slow training on TPU.\n\nThe part of only padding to the longest leads to slow training applies to pytorch as well.\nTry pad_to_multiple_of=8 to curb the variance of token length in batches.\n\ncollator = DataCollatorForSeq2Seq(tokenizer, padding=True, pad_to_multiple_of=8)\ndl = DataLoader(ds['train'].with_transform(lambda x: tokenizer(x['dialogue'])), batch_size=8, collate_fn=collator)\n\n\ntk_batched = np.array([batch['input_ids'].shape[-1] for batch in dl])\n\nprint(len(tk_batched), len(dl))\nprint(len(np.unique(tk_batched)))\n\nnp.unique(tk_batched).max(), np.unique(tk_batched).mean(), np.unique(tk_batched).min()\n\n1842 1842\n91\n\n\n(1160, 485.27472527472526, 96)\n\n\n1842 batches with 91 unique lengths, much better.\n\n\nTruncation experiment\nHow does truncation=True change anything? According to huggingface doc: tokenizer(batch_sentences, padding=True, truncation=True) has the same effect as tokenizer(batch_sentences, padding=True), both padding to max sequence in batch.\nLet’s try it out.\n\ncollator = DataCollatorForSeq2Seq(tokenizer, padding=True, pad_to_multiple_of=8)\ndl = DataLoader(ds['train'].with_transform(lambda x: tokenizer(x['dialogue'], truncation=True)), batch_size=8, collate_fn=collator)\n\n\ntk_batched = np.array([batch['input_ids'].shape[-1] for batch in dl])\n\nprint(len(tk_batched), len(dl))\nprint(len(np.unique(tk_batched)))\n\nnp.unique(tk_batched).max(), np.unique(tk_batched).mean(), np.unique(tk_batched).min()\n\n1842 1842\n51\n\n\n(512, 311.52941176470586, 96)\n\n\ntruncation=True in the tokenizer truncates the dialogue to 512 tokens, which is the max length of the T5. However, by default T5 should not have a set maximum length. This is imposed, artificial limitation by transformers library.\nBe careful to this behavior. Since unnoticed truncation means unnoticed loss input information during training.\n\n\nSource implementation\nIn source ipynb:\ntokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\nmax_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n\ndef preprocess_function(sample,padding=\"max_length\"):\n    # add prefix to the input for t5\n    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n\n    # tokenize inputs\n    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n    pass\n\nIt pads every input to absolute corpus max length. Would waste tons of memory and computation. The mean of dialogue is 149, meaning on average, 1k unnecessary tokens would be processed for each instance, and we have 14732 instances in training set.\nI use flan-t5 which is the heir of LM adopted T5, that makes prepending summarize: to the input not necessary."
  },
  {
    "objectID": "posts/2023/t5-finetuning-hello-world/samsum.html#training",
    "href": "posts/2023/t5-finetuning-hello-world/samsum.html#training",
    "title": "Hello, world! Huggingface T5 finetuning",
    "section": "Training",
    "text": "Training\n\nPrepare for trainer\n\n# no truncation, since the max_length in the training set is only 1153. Should be fine.\ndef preprocess(examples):\n    output = tokenizer(examples[\"dialogue\"])\n    output[\"labels\"] = tokenizer(examples[\"summary\"])[\"input_ids\"]\n    return output\n\n# tokenize the dataset\ntk_ds = ds.map(preprocess, batched=True).remove_columns(ds['train'].column_names)\n\n# load the evaluation metric\nrouge = evaluate.load('rouge')\n\n# postprocessing necessary for rouge\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds = [\n        \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds\n    ]\n    decoded_labels = [\n        \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n    ]\n\n    result = rouge.compute(\n        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n    )\n    return result\n\ncollator = DataCollatorForSeq2Seq(tokenizer, padding=True pad_to_multiple_of=8)\n\nargs = Seq2SeqTrainingArguments(\n    output_dir=model_output_dir,\n    evaluation_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    weight_decay=0.01,\n    save_total_limit=2,\n    num_train_epochs=1,\n    bf16=True,\n    gradient_accumulation_steps=4,\n    predict_with_generate=True,\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    hub_model_id=hub_model_id,\n    report_to=\"wandb\",\n)\n\n\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=args,\n    train_dataset=tk_ds[\"train\"],\n    eval_dataset=tk_ds[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=collator,\n    compute_metrics=compute_metrics,\n)\n\n\n\nFire up the training\n\ntrainer.train()\n\n\n\nCode\nwandb.finish()\n\ntotal_flos = trainer.state.total_flos\nruntime = trainer.state.log_history[1]['train_runtime']\nutilization = total_flos / 1e12 / runtime # in tflops"
  },
  {
    "objectID": "posts/2023/t5-finetuning-hello-world/samsum.html#result",
    "href": "posts/2023/t5-finetuning-hello-world/samsum.html#result",
    "title": "Hello, world! Huggingface T5 finetuning",
    "section": "Result",
    "text": "Result\nrouge-1: 47.8% is in the same range with the source blog. However, to save time it’s only trained for 1 epoch.\n\nAbout TFLOPS\n\nmodel.parallelize()\n\n20.43 tflops.\nPeak memory: GPU1: 16.6G, GPU2: 14.9G\n\nNo m.parallelize(), vanilla huggingface trainer.\n\n16.66 tflops.\nPeak memory: GPU1: 22.27, GPU2: 21.93G\nHigher GPU utilization, ~90%, slower training, more memory footprint. Why…?\n\npad_to_multiple_of=64 -> 19.72 tflops\n\nNot ready to innovate on dark magic yet LoL.\n\nNo pad_to_multiple_of=8 -> 20.38 tflops\n\nNo need to do this religiously. Make no difference with pytorch and this dataset."
  },
  {
    "objectID": "posts/2023/sc-cot/index.html",
    "href": "posts/2023/sc-cot/index.html",
    "title": "Self-consistency and chain of thought",
    "section": "",
    "text": "Wang et al. (2022)"
  },
  {
    "objectID": "posts/2023/sc-cot/index.html#reasoning",
    "href": "posts/2023/sc-cot/index.html#reasoning",
    "title": "Self-consistency and chain of thought",
    "section": "Reasoning",
    "text": "Reasoning\n\nScale has a reduced benefit for tasks in the Maths, Logical Reasoning, and Common Sense categories. Our results suggest that for certain flavours of mathematical or logical reasoning tasks, it is unlikely that scale alone will lead to performance breakthroughs. –(Rae et al. 2022)\n\nIt points out flat scaling curve of few task categories. Since then, google has been very creative to push the frontier with CoT (Wei et al. 2022), SC (Wang et al. 2022) and least to most (Zhou et al. 2022). CoT is the most exciting method to scale computation on tasks since few-shot in-context learning.\nInformal reasoning would be solved. DeepMind and OpenAI are all into solving formal reasoning, the last frontier wrt reasoning if AI could get logic and math right.\nCodex family model is the first step on solving formal reasoning. In SC and BBH (Suzgun et al. 2022), code-davinci-002 performs better than InstructGPT families on reasoning tasks. DeepMind even dives into GNN to explore architecture other than transformer. Reasoning in general would be solved as a modality in near future. It may require a specialized model, but would ultimately be fused into general LLM like image, audio and the like.\n\nThe approach to the irreducible loss does not necessarily indicate diminishing returns for representation quality or semantic content as significant semantic information may lie in the last few bits. –(Henighan et al. 2020)\n\nTo get natural language understanding right, scale is necessary. This also explains why CoT only works with scale. Small model makes too many semantic mistakes that render scaling computation with CoT worthless. SC could cancel out mistakes by majority vote to improve performance for model of all size but the increased computational cost far out weight possible gain for small model. Self-ensemble weak reasoner is a waste of resource."
  },
  {
    "objectID": "posts/2023/sc-cot/index.html#retrieval",
    "href": "posts/2023/sc-cot/index.html#retrieval",
    "title": "Self-consistency and chain of thought",
    "section": "Retrieval",
    "text": "Retrieval\nScale may not be the most effective method to solve world knowledge problem. 1T param model may get the last few bit of semantics but it won’t get the facts 100% right. That’s why retrieval is necessary. One could treat external knowledge database as one modality and figure out how to fuse it with general LLM.\nThink about how existing multimodal model fuses modalities, ex: Dall-E (Ramesh et al. 2022), Diffusion (Rombach et al. 2022), MusicLM (Agostinelli et al. 2023). RETRO (Borgeaud et al. 2022) is a great example of treating external memory as modality and fuse it with general LM deeply. Of course it’s not plug and play but still a very interesting direction.\nIn-context retrieval dominates current research output because of light resource requirement. Its value is similar to prompt engineering: the most effective method to probe LLM to find new gains, but prompt engineering would never be the ultimate solution. It’s a tentative exploration process. Like instruction finetuning makes LLM to follow human instruction and do CoT in 0 shot, rather than few shot, RETRO like solution may render methods such as recitaiton (Sun et al. 2022) unnecessary. However, recitation to me is like SC for open ended text generation, which is one great first step into retrieval world by scaling computation on retrieval tasks, like CoT to rationale engineering."
  },
  {
    "objectID": "posts/2023/sc-cot/index.html#multimodal",
    "href": "posts/2023/sc-cot/index.html#multimodal",
    "title": "Self-consistency and chain of thought",
    "section": "Multimodal",
    "text": "Multimodal\n500b+ dense LLM, 1T+ MoE, text-davinci-003 are great and useful but not enough. Have to find a way to fuse modalities. Small model like T5-11b, yes 11b is the new small lol, is still important for controlling latency and cost. Imagine doing 40 path SC on a 540b model per response for interactive UX. Not ideal. A good production example: Neeva1.1 T5 for serving ChatGPT like search\nMultimodal is on fire. One big end to end model may be enough, ex: Gato (Reed et al. 2022). On the other hand, modular approach with glue architecture may work, ex: Flamingo (Alayrac et al. 2022) and RETRO. It’s great to be alive in this era of AI."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "🖖 Welcome",
    "section": "",
    "text": "Hello, world! Huggingface T5 finetuning\n\n\n\n\n\n\n\ntutorial\n\n\ncopy\n\n\n\n\nHow to finetune Flan-T5 with samsum dataset?\n\n\n\n\n\n\nFeb 10, 2023\n\n\nLucas Shen\n\n\n\n\n\n\n  \n\n\n\n\nHumor in AI\n\n\n\n\n\n\n\ngrounding\n\n\nmultimodal\n\n\n\n\nComputational humor?\n\n\n\n\n\n\nFeb 5, 2023\n\n\nLucas Shen\n\n\n\n\n\n\n  \n\n\n\n\nSelf-consistency and chain of thought\n\n\n\n\n\n\n\nrationale engineering\n\n\n\n\nLesson learned from CoT and SC.\n\n\n\n\n\n\nFeb 5, 2023\n\n\nLucas Shen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "One pressing question woke him up every morning, as regularly as the screech of the whistle of the train that chugged by his cabin, on tracks built just up the hill from Walden Pond, where he’d hoped to still his soul. Were all these vast designs and rapid strides worth it? Thoreau thought not. He came to this truth: “They are but improved means to an unimproved end.”And still the trains chugged along, and the factories hummed, and the banks opened and closed, and the presses printed newspapers, and the telegraph wires reached across the nation, in one great and unending thrum.\n– Jill Lepore, These Truths: A History of the United States\n\nI’m Lucas Shen, independent researcher interested in cryptography and AI.\n\nCryptography enables pseudonymity and information exchange without centralized point of failure.\n\nAI enhances and complements human.\n\nTogether, with decent personal computation power, autonomy could be affordable at scale.\nnostr: npub1tk5wx0cgsthysderdmr5f2pm5yc094qlsr94l2tcy7dyp7hararqdy62tt"
  }
]