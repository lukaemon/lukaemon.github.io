[
  {
    "objectID": "posts/2023/learning-to-learn/index.html",
    "href": "posts/2023/learning-to-learn/index.html",
    "title": "Learning to learn",
    "section": "",
    "text": "M.C. Escher‚Äôs ‚ÄúDrawing Hands,‚Äù a lithograph from 1948. Collection of Experience in Visual Arts, Cordon Art B.V., Baarn, The Netherlands.\nLearning is change, mentally and physically. Learning is creation.\nFirst, one creates new mental constructs, represented as new neuronal firing patterns. Over time, firing patterns change actual wirings of the brain.\nSecond, one creates various artifacts to realize learned ideas. Artifacts like poem, book, music, painting, sculpture, architecture, computer program, movie, even video game. The medium is the message. The medium is the host of idea cross pollination cascades. Beautiful and scary ideas are born and died is such infosphere.\nCould alway try harder, but time and mental energy can‚Äôt be leveraged. Maybe stepping back once for a while to think about how to learn more effectively would be constructive."
  },
  {
    "objectID": "posts/2023/learning-to-learn/index.html#structure-and-interpretation-of-computer-programs",
    "href": "posts/2023/learning-to-learn/index.html#structure-and-interpretation-of-computer-programs",
    "title": "Learning to learn",
    "section": "Structure and Interpretation of Computer Programs",
    "text": "Structure and Interpretation of Computer Programs\nIn the first great SICP1 course2, the professor shares how to learn a new computer language. I believe it applies to all learnings. 3 simple principles are key to manage complexity, experiment and build new things:1¬†2¬†youtube: 2 min\n\nPrimitive elements\n\nMeans of combination\n\nMeans of abstraction\n\nPrimitive elements are fundamental building block, ex: lego.\nMeans of combination are API between elements. One could fit different building blocks together for something new. ex: fit variety of lego blocks to build a lego house.\nMeans of abstraction is where magic happened. How to name a complex construct and use it as primitives in later builds? Building with primitive elements are building from first principle. It offers both maximum flexibility and maximum redundancy. To build a lego city, if one could name a lego house and use it as primitive to expand the lego city, it would be way more efficient than building every single house from scratch.\nMastering primitives, APIs and the power of abstraction, complex system could be understood, and created."
  },
  {
    "objectID": "posts/2023/learning-to-learn/index.html#the-nature-of-creation-is-remix",
    "href": "posts/2023/learning-to-learn/index.html#the-nature-of-creation-is-remix",
    "title": "Learning to learn",
    "section": "The nature of creation is remix",
    "text": "The nature of creation is remix\n\n\n\nEverything is a Remix: part1, part2, part3\n\n\nThe series do an incredible job to explain the nature of creation from cultural perspective. Artists first would copy lots of previous works to learn the basics and build up muscle memories, primitive elements. Then they could start tweaking old ideas, and fit different ideas together to create new one, means of combination and abstraction.\nThere would be countless failures during all stages, copy, transform and combine. Typical learning process. One just have to find the grit."
  },
  {
    "objectID": "posts/2023/learning-to-learn/index.html#evergreen-notes-and-spaced-repetition",
    "href": "posts/2023/learning-to-learn/index.html#evergreen-notes-and-spaced-repetition",
    "title": "Learning to learn",
    "section": "Evergreen notes and spaced repetition",
    "text": "Evergreen notes and spaced repetition\nThe idea of evergreen notes3 is keeping notes on ideas, let them evolve and connect. The process could easily be supported by software such as Obsidian4.3¬†Andy Matuschak - Evergreen notes4¬†obsidian.md\nThink of every idea as primitive. For example, treat the idea about learning to learn as atomic. If one could keep notes about learning to learn whenever it‚Äôs triggered and connect it to other ideas, the knowledge graph would become organic. It will function like a mental fusion reactor, reinforcing one idea while making connections to others, even trigger conflicts among ideas. This is super valuable process to sustain. Especially treasure and cultivate the internal conflicts.\n\nThe test of a first-rate intelligence is the ability to hold two opposed ideas in mind at the same time and still retain the ability to function.\nF. Scott Fitzgerald\n\nSpaced repetition5 resonates well with understanding primitives and the copy phase. Find the rhythm to reinforce fundamentals and grow the network density.5¬†Andy Matuschak - How can we develop transformative tools for thought?\nIt could be unintentional repetition or deliberate practice6. Experiment and find the right mix.6¬†wiki"
  },
  {
    "objectID": "posts/2023/learning-to-learn/index.html#bootstrapping-with-delta",
    "href": "posts/2023/learning-to-learn/index.html#bootstrapping-with-delta",
    "title": "Learning to learn",
    "section": "Bootstrapping with \\(\\Delta\\)",
    "text": "Bootstrapping with \\(\\Delta\\)\nWhat to copy? To transform? To combine with? I know nothing right now. How to improve the quality of idea?\nThe quality is conditioned on:\n\nHow good is the world model?\nHow goodness of world model attenuate during n step roll out? What‚Äôs the largest, usable n?\nHow fast, how many orthogonal possibilities could be found based on n-step rolled out simulations?\nPrecision and recall?\n\nPrecision: the ratio of good idea.\nRecall: out of all good possible ideas, how many are found?\n\n\nGenerate 100 bad ideas to get a good one is ok at the beginning, but sticking to naive working hard is never ideal. To improve, one needs to improve the world model, the quality of simulation and the efficiency of exploration.\nGreat mentors and brilliant peers are a bliss. However, solo walker could still make meaningful strides. The key is to expand on time axis and create delta to answer known unknown and explore unknown unknown.\nLearning by doing is effective to create delta. One should adapt read-do-read-do cycle, instead of read-read-read-do. Creating delta is similar to debugging computer program. Create breakpoints, log tiered info, analyze log, error message and abnormal behavior.\nStarting every new project by writing down the context and expectation. The goal is to create a t_0 snapshot.\n\nTrigger of the project? Why?\nExpected result?\nExpected learning?\nExpected obstacles?\n\nDuring the project, create an operational log. The goal is to create range(t_1, t_n) snapshots.\n\nLog new ideas about how to improve the project, possible extension, connection to other projects, follow up, even tangential quantum leap. Don‚Äôt care about the quality. Just log.\nLog questions. One should have many questions, confusions pop out during the project. Log them all.\nLog tentative realizations.\n\nThe whole project is an intensive self-question-answering process. Like be your own psychiatrist to debug your own mind. One should generate result and an operational log as side effects. Finish the project and critique snapshots.\nCritique of the context is the largest delta. One may enter the project for all the wrong reasons and with many biased or varied expectations. Hopefully one could recognize some and figure out why looking backward.\nCritique of logs creates a flow of delta along the progress. Hopefully one could see how old perspective changes and new form. One could ask old questions sharper, and start asking new questions.\nThat delta is THE force. Actively search, mine, recognize, accept and adopt to it.\nLearning is a process of resolving merge conflicts between internal world model and perceived reality. Faster learning means one is willing to and could resolve more conflicts in a shorter time. Stopped learning means one is unwilling to or unable to see the delta and resolve the conflict.\nBootstrapping is soliloquy by nature. Endless self-play. It could be a long gradually with no light of suddenly, but one can choose to believe.\nFor most adults, multiagent learning is more humane. However, if one could effectively copy, transform and simulate different perspectives through various media such as papers or books, some of real human apprenticeship or peers could be replaced by self-multiagent learning, ideally saving communication overhead and redundant information. This is similar to self-ensemble on LLM conditioned by different prompts.\nIt‚Äôs possible to be your own mentor, simulate the diversified peers and grow sustainably. Just have to reset and self-question all the time to keep many characters alive in one brain. Not the most effective route though. One will learn way more faster by simply observing brilliant people. Deep collaboration has even greater yields. Closed mode can‚Äôt match such electrifying environment, but slow learning is real, honest and grounded."
  },
  {
    "objectID": "posts/2023/learning-to-learn/index.html#why",
    "href": "posts/2023/learning-to-learn/index.html#why",
    "title": "Learning to learn",
    "section": "Why",
    "text": "Why\nThese ideas are in the same league of every other how to knowledge, means to ends. Humans build tools to achieve goals. However, developing the understanding of how is as important as the understanding of why.\n\nHe who has a why to live can bear almost any how.\n‚Äì Friedrich Nietzsche\n\nHappy learning and remember to ask periodically: to what ends?"
  },
  {
    "objectID": "posts/2023/humor-ai/index.html",
    "href": "posts/2023/humor-ai/index.html",
    "title": "Humor AI",
    "section": "",
    "text": "MJ: abstract art, picasso‚Äôs expression about humor ‚Äìar 16:9\n\n\nHumor is preconditioned on the ability to see the bright side of something. One has to be able to see many sides, consciously choose the funny, optimistic interpretation and express in a way that resonates with target audiences. It shows both raw intelligence and wisdom.\nTo detect humor and be humorous, grounding is necessary. Grounding to me is weaving modalities. Just a fancy way of saying having sampled variety of experiences of certain things or events.\nFor example, to know what it really means about apple, one could write ‚Äúapple‚Äù, read about it, draw, hold, throw, smell, eat, plant, cook, even share it with others. Without grounding, one can‚Äôt have acute and diversified perspectives on a thing or an event. It would be very hard to see ironic yet optimistic interpretation, be it human or AI.\nGrounding AI to full set of human experience is aligning computational humor to humans‚Äô. They may be able to see a kind of digital humor that is a bridge too far for us. Literally why Samantha in Her leaving Theodore1.1¬†youtube\nHumor is the ultimate Turing test. I see this as the source of Yann‚Äôs recent debate with others2. It‚Äôs the most difficult test to pass. It‚Äôs the most difficult test to create.2¬†tweet"
  },
  {
    "objectID": "posts/2023/from-text-to-multimodal/index.html",
    "href": "posts/2023/from-text-to-multimodal/index.html",
    "title": "From vanilla text encoder decoder to multimodal mid fusion attention bottleneck",
    "section": "",
    "text": "midjourney: fusion of multimodal sensory information ‚Äìar 16:9\nTracking the evolution trajectory from Vaswani et al. (2017) to cutting edge multimodal model (Nagrani et al. 2022) is an interesting journey. I‚Äôm mostly interested in multimodal fusion, meaning how to fuse information from different sensory modalities. A juxtaposition of 2 evolution tracks speaks volumes.\nTo better appreciate the juxtaposition, a compare and contrast between the starting point and the cutting edge would be helpful."
  },
  {
    "objectID": "posts/2023/from-text-to-multimodal/index.html#attention-is-all-you-need",
    "href": "posts/2023/from-text-to-multimodal/index.html#attention-is-all-you-need",
    "title": "From vanilla text encoder decoder to multimodal mid fusion attention bottleneck",
    "section": "Attention is all you need",
    "text": "Attention is all you need\n\n\n\nVaswani et al. (2017)\n\n\nStaring at this seminal encoder decoder architecture long enough makes me realize few things:\n\nAttention is applied to a set, not strictly on sequence. Very general and versatile inductive bias.\nSequence is just the result of information serialization. Text is natural sequence. Image tokens could be rasterized. Most of seq2seq magics are actually set2set plus optional positional information, such add-on info could be of many kinds.\nThe whole encoder stack plus the cross attention is an adapter module (Pfeiffer et al. 2023) to condition an autoregressive generative decoder stack.\nThe generative model doesn‚Äôt have to be autoregressive, or about text. The gold is multimodal representation learning. Generation is just one task.\nThe power of transformers applies to multimodal fusion. Text is just the beginning."
  },
  {
    "objectID": "posts/2023/from-text-to-multimodal/index.html#attention-bottleneck",
    "href": "posts/2023/from-text-to-multimodal/index.html#attention-bottleneck",
    "title": "From vanilla text encoder decoder to multimodal mid fusion attention bottleneck",
    "section": "Attention bottleneck",
    "text": "Attention bottleneck\nInspired by this great presentation1 from Jack Rae, I realize that different form of bottleneck, ex: global workspace theory (Baars 2017), could be interpreted as different expression of Occam‚Äôs razor, which has deep root in information theory that regards effective compression as general intelligence.1¬†Compression for AGI - Jack Rae\nCopy and paste is just brute force. Information bottleneck is suffering that builds intelligence.\nNagrani et al. (2022) introduces bottleneck mid fusion. I see it as successor to Gato‚Äôs (Reed et al. 2022) all you can eat style early fusion. This is a beautiful implementation of using bottleneck to foster learning intelligence.\n\n\n\nNagrani et al. (2022)\n\n\nThe beauty of bottleneck mid fusion comes from few realizations about multimodal learning:\n\n\nvariations in learning dynamics between modalities\ndifferent noise topologies, with some modality streams containing more information for the task at hand than others\nspecialized input representations.\n\n‚ÄìNagrani et al. (2022)\n\nResonate pretty well with Rombach et al. (2022)\n\nMost bits of a digital image correspond to imperceptible details. While DMs allow to suppress this semantically meaningless information by minimizing the responsible loss term, gradients (during training) and the neural network backbone (training and inference) still need to be evaluated on all pixels, leading to superfluous computations and unnecessarily expensive optimization and inference. We propose latent diffusion models (LDMs) as an effective generative model and a separate mild compression stage that only eliminates imperceptible details.\n\nYou can see that noise topology exemplified with image. Bits are not equal to information, and human civilization builds semantics only on the subset of all information. Multimodal learning has to deal with different noise topology among different modalities. That‚Äôs why early fusion could work, but it would be far from Pareto frontier.\nbottleneck mid fusion is making Pareto improvement by doing mid fusion with information bottleneck. Mid fusion affords different modality independent computation to weed out noises. The context window of bottleneck token is global workspace, on which joint representation could be learned. The hope is quality of joint representation could be foster by imposed limitation.\nWith basic understandings about the baseline and cutting edge, moving on to juxtaposition."
  },
  {
    "objectID": "posts/2023/from-text-to-multimodal/index.html#text-2017-og-encoder-decoder-hyperscaled-decoder",
    "href": "posts/2023/from-text-to-multimodal/index.html#text-2017-og-encoder-decoder-hyperscaled-decoder",
    "title": "From vanilla text encoder decoder to multimodal mid fusion attention bottleneck",
    "section": "Text: 2017 OG encoder-decoder > hyperscaled decoder",
    "text": "Text: 2017 OG encoder-decoder > hyperscaled decoder\nText2text treat input text as a modality. Info of input text is fused to text generative model to decide what to generate next. Encoder decoder architecture is basically a forced sparsity that implements a form of mid fusion and information bottleneck.\nThe whole encoder stack is isolated representation learning of input text. Regardless how many input tokens, as long as they are within context window, the output is a learned representation, a vector in latent space. Text to vector is a huge bottleneck. Useful information are encoded in the topological relationships in such latent space.\nFor example, the T5 model takes input text and transforms it into a learned representation for the decoder stack. This transformation involves mapping a (number_of_token, d_model) tensor to a (d_model) tensor where d_model is 768 according to the model‚Äôs configuration2. The input goes through several transformer layers before producing this vector.2¬†config.json\nThe model‚Äôs bottleneck is in the encoder stack, where the input text is transformed into a 768-dimensional latent space. Each input text is represented as a unique vector in this space, and the model‚Äôs intelligence depends on the relationships between these vectors, particularly for input texts that the model has not seen before.\nRepresentation is later used in cross-attention to contribute relevant information to the text generation stack. The relevancy is conditioned on generated text and the relevant info added to the output of masked attention\n\nAll modular computation functions can be reduced to function composition: the output of the function \\(f_{\\theta_i}\\) of a model is added to a new term that depends on a learned function \\(f_{\\varphi}\\): \\(f'_i(x) = f_{\\theta}(x) + f_{\\varphi_i}(x)\\).\n‚ÄìPfeiffer et al. (2023)\n\nYou can see masked attention + cross attention is that addition. The purpose of the whole encoder stack and cross attention is to infuse input info to change how text generative function works. This is what adapters do.\nWhy don‚Äôt we have 100b+ T5, but many 100b+ decoder only LLM?\nText only modality doesn‚Äôt deal with different noise topology. It has one learning dynamics with coherent input representation. Forced mid fusion is unnecessary engineering. That‚Äôs why independent encoder is redundant especially during the push of hyperscaling to squeeze out the last few bits.\n\n‚Ä¶there are benefits to squeezing as much performance as possible out of large generative image models, as significant semantic information may lie in the last few bits\n‚ÄìHenighan et al. (2020)\n\nSmall model (<20b) is not as powerful as 100b+ model. Forced sparsity could be useful inductive bias to boost performance to a point. That‚Äôs why T5 works great for finetuning (Wang et al. 2022; Longpre et al. 2023). It‚Äôs meant to be specialized. However, when the goal is foundation model that saturates one modality as much as possible, scale is one working formula and the raw performance is self-evident. Big models are more sample efficient, learn faster and compress better.\n\n\n\nTouvron et al. (2023)"
  },
  {
    "objectID": "posts/2023/from-text-to-multimodal/index.html#multimodal-encoder-decoder-and-early-fusion-decoder",
    "href": "posts/2023/from-text-to-multimodal/index.html#multimodal-encoder-decoder-and-early-fusion-decoder",
    "title": "From vanilla text encoder decoder to multimodal mid fusion attention bottleneck",
    "section": "Multimodal: encoder-decoder and early fusion decoder",
    "text": "Multimodal: encoder-decoder and early fusion decoder\nIt would be easier to see the basic pattern with few examples.\n# patterns\nfused_info = fuse(modality_a, modality_b)\noutput = task(fused_info)\nTask itself is not that important. The point is learnable intelligence to get useful multimodal representation. Task is used to gauge whether the learned representation is useful.\n\nRETRO: enc-dec text|text, text generation\n\n\n\nBorgeaud et al. (2022)\n\n\nExternal memory as modality to power traditional text generation task. Retrieval as first class citizen not after thought as in-context retrieval (Mialon et al. 2023).\n\n\nFlamingo: enc-dec image|text, text generation\n\n\n\nAlayrac et al. (2022)\n\n\nInterleaving image and text. VQA is the logical next step of ChatGPT.\n\n\nlatent diffusion: enc-dec image|text, image generation\n\n\n\nRombach et al. (2022)\n\n\nConditioned diffusion for image generation.\n\n\nMuse: image|text, image generation\n\n\n\nChang et al. (2023)\n\n\nLike 2 layer enc-dec, diffusion free image generation. Very cool.\n\n\nSpeechT5: enc-dec audio|text, text <-> audio generation\n\n\n\nAo et al. (2022)\n\n\nPre-post processing modules are adapters. Close to the idea:\n\n‚Ä¶ image representations from vision models can be transferred as continuous prompts to frozen LMs by training only a single linear projection.\n‚ÄìMerullo et al. (2022)\n\n\n\nGato: decoder only all2all\n\n\n\nReed et al. (2022)\n\n\nA 1.2B parameter decoder-only transformer with 24 layers, an embedding size of 2048, and a post-attention feedforward hidden size of 8196.\nThe major contributions are multimodal tokenization, embedding and training objectives. Pretty sure DeepMind didn‚Äôt expect naive early fusion decoder to be the final answer of multimodal learning.\nOr it‚Äôs that easy‚Ä¶ Just few missing pieces to deal with such as limited context window, quadruple computation of self-attention and so on. Given recent light speed of AI progress, I don‚Äôt think Gato is the answer but I‚Äôm not so sure anymore LoL.\n\nAll above are interesting. The holy grail is to find a scaling friendly formula for multimodal learning just like GPT3 did to text. Scaling friendly in terms of:\n\nSimple, easy, parallelizable architecture.\nBIG data.\nSelf supervised objective function.\n\nCurrent multimodal models are far from settling down on the architecture. Image caption dataset is great but one simply can‚Äôt expect to find enough (x, natural language) supervision for every modality. Too many hard to describe intelligence going on daily. Try to explain the internal state of playing piano or the touch of a perfect golf swing. There would always be some modalities that have small cross-modal latent representation. Language is useful but not the universal multimodal substrate.\nI just don‚Äôt see current mutation of multimodal enc-dec or simple multimodal decoder architecture are there yet. To be fair, some are already very useful."
  },
  {
    "objectID": "posts/2023/from-text-to-multimodal/index.html#back-to-bottleneck-mid-fusion",
    "href": "posts/2023/from-text-to-multimodal/index.html#back-to-bottleneck-mid-fusion",
    "title": "From vanilla text encoder decoder to multimodal mid fusion attention bottleneck",
    "section": "Back to bottleneck mid fusion",
    "text": "Back to bottleneck mid fusion\nBoth architecture and choice of modalities are interesting.\n\narchitecture\n\n\n\nNagrani et al. (2022)\n\n\nIntuitively, late fusion and early fusion can‚Äôt be optimal. Remember we want to scale like crazy. Manually engineered enc-dec is not optimal as well. The paper use few bottleneck token and bottleneck transformer to replace the whole encoder plus cross attention to decoder. The size of bottleneck token could be a hyperparameter to control the size of cross-modal latent space as in Ao et al. (2022).\nThe architecture is simple and elegant.\n\n\nmodality\nGiven the following context:\n\nText is done. Mobile phone kind of done that innovations are still expected but the field is a perfect competition market. The speed, price and quality of ChatGPT API is crazy.3\n(text, image) -> image is in the instruction finetuning stage (Zhang and Agrawala 2023; L. Huang et al. 2023).\n\n(text, image) -> text is improving super fast (Koh, Salakhutdinov, and Fried 2023; Li et al. 2023; S. Huang et al. 2023)\n\n3¬†Introducing ChatGPT and Whisper APIs(image, audio) -> x is a good proxy to think about how and what <12 month baby could learn. It is right at the multimodal research frontier. I see this as the last frontier before embodied learning and some even argue body is not necessary for high intelligence. Reviewing my daily activity, input and output, the idea of body is not necessary for average level intelligence finds support lol.\nVideo is abundant. Learning dynamics and noise topology of image and audio are real challenges. Yes we could use natural language to bridge them but that is boring and even it works to an extend, better solution is expected."
  },
  {
    "objectID": "posts/2023/from-text-to-multimodal/index.html#outro",
    "href": "posts/2023/from-text-to-multimodal/index.html#outro",
    "title": "From vanilla text encoder decoder to multimodal mid fusion attention bottleneck",
    "section": "Outro",
    "text": "Outro\nI‚Äôm facing 2 problems to further exploring this track of research:\n\nWeak information theory understanding.\nWeak engineering capabilities to carry on experiments effectively.\n\nAghajanyan et al. (2023) could help me get oriented to information theory wrt multimodal research but the theoretical gap is too big to bridge with just one paper. Time to read textbooks.\nEnd of procrastination. Back to code and read."
  },
  {
    "objectID": "posts/2023/t5-finetuning-hello-world/samsum.html",
    "href": "posts/2023/t5-finetuning-hello-world/samsum.html",
    "title": "Hello, world! Huggingface T5 finetuning",
    "section": "",
    "text": "MJ: computer scientist coding to train AI model, studio ghibli ‚Äìar 16:9 ‚Äìniji\nA learning note from reproducing this amazing post by Philipp Schmid."
  },
  {
    "objectID": "posts/2023/t5-finetuning-hello-world/samsum.html#load-dataset",
    "href": "posts/2023/t5-finetuning-hello-world/samsum.html#load-dataset",
    "title": "Hello, world! Huggingface T5 finetuning",
    "section": "Load dataset",
    "text": "Load dataset\nsamsum is a conversation dataset. The goal is to summarize a conversation. Dataset is available on Huggingface.\n\nds = load_dataset(dataset_name)\nds\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 818\n    })\n})\n\n\n\nexample = ds[\"train\"][0]\nexample\n\n{'id': '13818513',\n 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}"
  },
  {
    "objectID": "posts/2023/t5-finetuning-hello-world/samsum.html#max_length-analysis",
    "href": "posts/2023/t5-finetuning-hello-world/samsum.html#max_length-analysis",
    "title": "Hello, world! Huggingface T5 finetuning",
    "section": "max_length analysis",
    "text": "max_length analysis\nInvestigate truncation and padding to get statistics on dialogue and summary token length.\nOutlier long input may cause out of memory error during training.\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\nmodel.parallelize()\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n\ntk_dialogue = tokenizer(ds[\"train\"][\"dialogue\"])[\"input_ids\"]\ntk_summary = tokenizer(ds[\"train\"][\"summary\"])[\"input_ids\"]\npd.set_option('display.float_format', lambda x: '%.1f' % x)\n\ndf = pd.DataFrame(\n    {\"dialogue\": [len(d) for d in tk_dialogue], \"summary\": [len(s) for s in tk_summary]}\n)\nprint(df.describe())\n\n       dialogue  summary\ncount   14732.0  14732.0\nmean      149.0     28.9\nstd       110.7     15.1\nmin         1.0      2.0\n25%        66.0     17.0\n50%       120.0     26.0\n75%       202.0     37.0\nmax      1153.0     94.0\n\n\nMy first hunch is I shouldn‚Äôt truncate the input. Just need to pad to the longest of the batch. The setting would be tokenizer(batch_sentences, padding=True).\nHowever, it seems that truncation is inevitable in production. You need to find a balance and curb the long input outlier.\nFor this dataset, 1153 max is not too crazy.\n\nPadding experiments\nLet‚Äôs experiment with different padding strategy and how it affects the following batching and training.\nFirst, do it without truncation:\n\ntk_dialogue = tokenizer(ds[\"train\"][\"dialogue\"], padding=True)[\"input_ids\"]\ntk_summary = tokenizer(ds[\"train\"][\"summary\"], padding=True)[\"input_ids\"]\npd.set_option('display.float_format', lambda x: '%.1f' % x)\n\ndf = pd.DataFrame(\n    {\"dialogue\": [len(d) for d in tk_dialogue], \"summary\": [len(s) for s in tk_summary]}\n)\nprint(df.describe())\n\n       dialogue  summary\ncount   14732.0  14732.0\nmean     1153.0     94.0\nstd         0.0      0.0\nmin      1153.0     94.0\n25%      1153.0     94.0\n50%      1153.0     94.0\n75%      1153.0     94.0\nmax      1153.0     94.0\n\n\nExpected result. This is literally treating the whole training corpus as one full batch. All sequences are pad to the max length, 1153.\nTry this idea with batch_size = 8 in dataloader.\n\nfrom torch.utils.data import DataLoader\n\ncollator = DataCollatorForSeq2Seq(tokenizer, padding=True)\ndl = DataLoader(ds['train'].with_transform(lambda x: tokenizer(x['dialogue'])), batch_size=8, collate_fn=collator)\n\n\ntk_batched = np.array([batch['input_ids'].shape[-1] for batch in dl])\n\nprint(len(tk_batched), len(dl))\nprint(len(np.unique(tk_batched)))\n\nnp.unique(tk_batched).max(), np.unique(tk_batched).mean(), np.unique(tk_batched).min()b\n\n1842 1842\n482\n\n\n(1153, 389.02904564315355, 92)\n\n\n1842 batches, with 482 unique length. This is fine for pytorch but would be brutal for jax jit since every change of input shape would trigger jit recompilation.\n\nIf training on TPU, it is recommended to pad all examples of the dataset to the same length or make use of pad_to_multiple_of to have a small number of predefined bucket sizes to fit all examples in. Dynamically padding batches to the longest example is not recommended on TPU as it triggers a recompilation for every batch shape that is encountered during training thus significantly slowing down the training. only padding up to the longest example in a batch) leads to very slow training on TPU.\n\nThe part of only padding to the longest leads to slow training applies to pytorch as well.\nTry pad_to_multiple_of=8 to curb the variance of token length in batches.\n\ncollator = DataCollatorForSeq2Seq(tokenizer, padding=True, pad_to_multiple_of=8)\ndl = DataLoader(ds['train'].with_transform(lambda x: tokenizer(x['dialogue'])), batch_size=8, collate_fn=collator)\n\n\ntk_batched = np.array([batch['input_ids'].shape[-1] for batch in dl])\n\nprint(len(tk_batched), len(dl))\nprint(len(np.unique(tk_batched)))\n\nnp.unique(tk_batched).max(), np.unique(tk_batched).mean(), np.unique(tk_batched).min()\n\n1842 1842\n91\n\n\n(1160, 485.27472527472526, 96)\n\n\n1842 batches with 91 unique lengths, much better.\n\n\nTruncation experiment\nHow does truncation=True change anything? According to huggingface doc: tokenizer(batch_sentences, padding=True, truncation=True) has the same effect as tokenizer(batch_sentences, padding=True), both padding to max sequence in batch.\nLet‚Äôs try it out.\n\ncollator = DataCollatorForSeq2Seq(tokenizer, padding=True, pad_to_multiple_of=8)\ndl = DataLoader(ds['train'].with_transform(lambda x: tokenizer(x['dialogue'], truncation=True)), batch_size=8, collate_fn=collator)\n\n\ntk_batched = np.array([batch['input_ids'].shape[-1] for batch in dl])\n\nprint(len(tk_batched), len(dl))\nprint(len(np.unique(tk_batched)))\n\nnp.unique(tk_batched).max(), np.unique(tk_batched).mean(), np.unique(tk_batched).min()\n\n1842 1842\n51\n\n\n(512, 311.52941176470586, 96)\n\n\ntruncation=True in the tokenizer truncates the dialogue to 512 tokens, which is the max length of the T5. However, by default T5 should not have a set maximum length. This is imposed, artificial limitation by transformers library.\nBe careful to this behavior. Since unnoticed truncation means unnoticed loss input information during training.\n\n\nSource implementation\nIn source ipynb:\ntokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\nmax_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n\ndef preprocess_function(sample,padding=\"max_length\"):\n    # add prefix to the input for t5\n    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n\n    # tokenize inputs\n    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n    pass\n\nIt pads every input to absolute corpus max length. Would waste tons of memory and computation. The mean of dialogue is 149, meaning on average, 1k unnecessary tokens would be processed for each instance, and we have 14732 instances in training set.\nI use flan-t5 which is the heir of LM adopted T5, that makes prepending summarize: to the input not necessary."
  },
  {
    "objectID": "posts/2023/t5-finetuning-hello-world/samsum.html#training",
    "href": "posts/2023/t5-finetuning-hello-world/samsum.html#training",
    "title": "Hello, world! Huggingface T5 finetuning",
    "section": "Training",
    "text": "Training\n\nPrepare for trainer\n\n# no truncation, since the max_length in the training set is only 1153. Should be fine.\ndef preprocess(examples):\n    output = tokenizer(examples[\"dialogue\"])\n    output[\"labels\"] = tokenizer(examples[\"summary\"])[\"input_ids\"]\n    return output\n\n# tokenize the dataset\ntk_ds = ds.map(preprocess, batched=True).remove_columns(ds['train'].column_names)\n\n# load the evaluation metric\nrouge = evaluate.load('rouge')\n\n# postprocessing necessary for rouge\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    decoded_preds = [\n        \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds\n    ]\n    decoded_labels = [\n        \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n    ]\n\n    result = rouge.compute(\n        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n    )\n    return result\n\ncollator = DataCollatorForSeq2Seq(tokenizer, padding=True pad_to_multiple_of=8)\n\nargs = Seq2SeqTrainingArguments(\n    output_dir=model_output_dir,\n    evaluation_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    weight_decay=0.01,\n    save_total_limit=2,\n    num_train_epochs=1,\n    bf16=True,\n    gradient_accumulation_steps=4,\n    predict_with_generate=True,\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    hub_model_id=hub_model_id,\n    report_to=\"wandb\",\n)\n\n\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=args,\n    train_dataset=tk_ds[\"train\"],\n    eval_dataset=tk_ds[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=collator,\n    compute_metrics=compute_metrics,\n)\n\n\n\nFire up the training\n\ntrainer.train()\n\n\n\nCode\nwandb.finish()\n\ntotal_flos = trainer.state.total_flos\nruntime = trainer.state.log_history[1]['train_runtime']\nutilization = total_flos / 1e12 / runtime # in tflops"
  },
  {
    "objectID": "posts/2023/t5-finetuning-hello-world/samsum.html#result",
    "href": "posts/2023/t5-finetuning-hello-world/samsum.html#result",
    "title": "Hello, world! Huggingface T5 finetuning",
    "section": "Result",
    "text": "Result\nrouge-1: 47.8% is in the same range with the source blog. However, to save time it‚Äôs only trained for 1 epoch.\n\nAbout TFLOPS\n\nmodel.parallelize()\n\n20.43 tflops.\nPeak memory: GPU1: 16.6G, GPU2: 14.9G\n\nNo m.parallelize(), vanilla huggingface trainer.\n\n16.66 tflops.\nPeak memory: GPU1: 22.27, GPU2: 21.93G\nHigher GPU utilization, ~90%, slower training, more memory footprint. Why‚Ä¶?\n\npad_to_multiple_of=64 -> 19.72 tflops\n\nNot ready to innovate on dark magic yet LoL.\n\nNo pad_to_multiple_of=8 -> 20.38 tflops\n\nNo need to do this religiously. Make no difference with pytorch and this dataset."
  },
  {
    "objectID": "posts/2023/sc-cot/index.html",
    "href": "posts/2023/sc-cot/index.html",
    "title": "Self-consistency and chain of thought",
    "section": "",
    "text": "Wang et al. (2022)"
  },
  {
    "objectID": "posts/2023/sc-cot/index.html#reasoning",
    "href": "posts/2023/sc-cot/index.html#reasoning",
    "title": "Self-consistency and chain of thought",
    "section": "Reasoning",
    "text": "Reasoning\n\nScale has a reduced benefit for tasks in the Maths, Logical Reasoning, and Common Sense categories. Our results suggest that for certain flavours of mathematical or logical reasoning tasks, it is unlikely that scale alone will lead to performance breakthroughs. ‚Äì(Rae et al. 2022)\n\nIt points out flat scaling curve of few task categories. Since then, google has been very creative to push the frontier with CoT (Wei et al. 2022), SC (Wang et al. 2022) and least to most (Zhou et al. 2022). CoT is the most exciting method to scale computation on tasks since few-shot in-context learning.\nInformal reasoning would be solved. DeepMind and OpenAI are all into solving formal reasoning, the last frontier wrt reasoning if AI could get logic and math right.\nCodex family model is the first step on solving formal reasoning. In SC and BBH (Suzgun et al. 2022), code-davinci-002 performs better than InstructGPT families on reasoning tasks. DeepMind even dives into GNN to explore architecture other than transformer. Reasoning in general would be solved as a modality in near future. It may require a specialized model, but would ultimately be fused into general LLM like image, audio and the like.\n\nThe approach to the irreducible loss does not necessarily indicate diminishing returns for representation quality or semantic content as significant semantic information may lie in the last few bits. ‚Äì(Henighan et al. 2020)\n\nTo get natural language understanding right, scale is necessary. This also explains why CoT only works with scale. Small model makes too many semantic mistakes that render scaling computation with CoT worthless. SC could cancel out mistakes by majority vote to improve performance for model of all size but the increased computational cost far out weight possible gain for small model. Self-ensemble weak reasoner is a waste of resource."
  },
  {
    "objectID": "posts/2023/sc-cot/index.html#retrieval",
    "href": "posts/2023/sc-cot/index.html#retrieval",
    "title": "Self-consistency and chain of thought",
    "section": "Retrieval",
    "text": "Retrieval\nScale may not be the most effective method to solve world knowledge problem. 1T param model may get the last few bit of semantics but it won‚Äôt get the facts 100% right. That‚Äôs why retrieval is necessary. One could treat external knowledge database as one modality and figure out how to fuse it with general LLM.\nThink about how existing multimodal model fuses modalities, ex: Dall-E (Ramesh et al. 2022), Diffusion (Rombach et al. 2022), MusicLM (Agostinelli et al. 2023). RETRO (Borgeaud et al. 2022) is a great example of treating external memory as modality and fuse it with general LM deeply. Of course it‚Äôs not plug and play but still a very interesting direction.\nIn-context retrieval dominates current research output because of light resource requirement. Its value is similar to prompt engineering: the most effective method to probe LLM to find new gains, but prompt engineering would never be the ultimate solution. It‚Äôs a tentative exploration process. Like instruction finetuning makes LLM to follow human instruction and do CoT in 0 shot, rather than few shot, RETRO like solution may render methods such as recitaiton (Sun et al. 2022) unnecessary. However, recitation to me is like SC for open ended text generation, which is one great first step into retrieval world by scaling computation on retrieval tasks, like CoT to rationale engineering."
  },
  {
    "objectID": "posts/2023/sc-cot/index.html#multimodal",
    "href": "posts/2023/sc-cot/index.html#multimodal",
    "title": "Self-consistency and chain of thought",
    "section": "Multimodal",
    "text": "Multimodal\n500b+ dense LLM, 1T+ MoE, text-davinci-003 are great and useful but not enough. Have to find a way to fuse modalities. Small model like T5-11b, yes 11b is the new small lol, is still important for controlling latency and cost. Imagine doing 40 path SC on a 540b model per response for interactive UX. Not ideal. A good production example: Neeva1.1¬†T5 for serving ChatGPT like search\nMultimodal is on fire. One big end to end model may be enough, ex: Gato (Reed et al. 2022). On the other hand, modular approach with glue architecture may work, ex: Flamingo (Alayrac et al. 2022) and RETRO. It‚Äôs great to be alive in this era of AI."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "üññ Welcome",
    "section": "",
    "text": "From vanilla text encoder decoder to multimodal mid fusion attention bottleneck\n\n\n\n\n\n\n\nmultimodal\n\n\n\n\nQuick note to recap what I‚Äôve learned recently about multimodal research\n\n\n\n\n\n\nMar 6, 2023\n\n\nLucas Shen\n\n\n\n\n\n\n  \n\n\n\n\nLearning to learn\n\n\n\n\n\n\n\nmeta-learning\n\n\n\n\nSo much to learn. How to learn more effectively?\n\n\n\n\n\n\nFeb 14, 2023\n\n\nLucas Shen\n\n\n\n\n\n\n  \n\n\n\n\nHello, world! Huggingface T5 finetuning\n\n\n\n\n\n\n\ntutorial\n\n\ncopy\n\n\n\n\nHow to finetune Flan-T5 with samsum dataset?\n\n\n\n\n\n\nFeb 10, 2023\n\n\nLucas Shen\n\n\n\n\n\n\n  \n\n\n\n\nHumor AI\n\n\n\n\n\n\n\ngrounding\n\n\nmultimodal\n\n\n\n\nComputational humor?\n\n\n\n\n\n\nFeb 5, 2023\n\n\nLucas Shen\n\n\n\n\n\n\n  \n\n\n\n\nSelf-consistency and chain of thought\n\n\n\n\n\n\n\nrationale engineering\n\n\n\n\nLesson learned from CoT and SC.\n\n\n\n\n\n\nFeb 5, 2023\n\n\nLucas Shen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "One pressing question woke him up every morning, as regularly as the screech of the whistle of the train that chugged by his cabin, on tracks built just up the hill from Walden Pond, where he‚Äôd hoped to still his soul. Were all these vast designs and rapid strides worth it? Thoreau thought not. He came to this truth: ‚ÄúThey are but improved means to an unimproved end.‚ÄùAnd still the trains chugged along, and the factories hummed, and the banks opened and closed, and the presses printed newspapers, and the telegraph wires reached across the nation, in one great and unending thrum.\n‚Äì Jill Lepore, These Truths: A History of the United States\n\nI‚Äôm an independent researcher interested in cryptography and AI.\n\n\n\nModern agrarian utopianism and the ideal form of AGI: Doraemon: Story of Seasons"
  }
]