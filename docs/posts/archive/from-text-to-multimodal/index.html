<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lucas Shen">
<meta name="dcterms.date" content="2023-03-06">
<meta name="description" content="Quick note to recap what I’ve learned recently about multimodal research">

<title>Lucas Shen - From vanilla text encoder decoder to multimodal mid fusion attention bottleneck</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../asset/favicon.jpg" rel="icon" type="image/jpeg">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta property="og:title" content="Lucas Shen - From vanilla text encoder decoder to multimodal mid fusion attention bottleneck">
<meta property="og:description" content="Quick note to recap what I’ve learned recently about multimodal research">
<meta property="og:image" content="https://lukaemon.github.io/posts/archive/from-text-to-multimodal/cover.png">
<meta property="og:site_name" content="Lucas Shen">
<meta property="og:image:height" content="896">
<meta property="og:image:width" content="1568">
<meta name="twitter:title" content="Lucas Shen - From vanilla text encoder decoder to multimodal mid fusion attention bottleneck">
<meta name="twitter:description" content="Quick note to recap what I’ve learned recently about multimodal research">
<meta name="twitter:image" content="https://lukaemon.github.io/posts/archive/from-text-to-multimodal/cover.png">
<meta name="twitter:image-height" content="896">
<meta name="twitter:image-width" content="1568">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Lucas Shen</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lukaemon"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/luka_emon"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">From vanilla text encoder decoder to multimodal mid fusion attention bottleneck</h1>
                  <div>
        <div class="description">
          Quick note to recap what I’ve learned recently about multimodal research
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">multimodal</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Lucas Shen </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 6, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#attention-is-all-you-need" id="toc-attention-is-all-you-need" class="nav-link active" data-scroll-target="#attention-is-all-you-need">Attention is all you need</a></li>
  <li><a href="#attention-bottleneck" id="toc-attention-bottleneck" class="nav-link" data-scroll-target="#attention-bottleneck">Attention bottleneck</a></li>
  <li><a href="#text-2017-og-encoder-decoder-hyperscaled-decoder" id="toc-text-2017-og-encoder-decoder-hyperscaled-decoder" class="nav-link" data-scroll-target="#text-2017-og-encoder-decoder-hyperscaled-decoder">Text: 2017 OG encoder-decoder &gt; hyperscaled decoder</a></li>
  <li><a href="#multimodal-encoder-decoder-and-early-fusion-decoder" id="toc-multimodal-encoder-decoder-and-early-fusion-decoder" class="nav-link" data-scroll-target="#multimodal-encoder-decoder-and-early-fusion-decoder">Multimodal: encoder-decoder and early fusion decoder</a>
  <ul class="collapse">
  <li><a href="#retro-enc-dec-texttext-text-generation" id="toc-retro-enc-dec-texttext-text-generation" class="nav-link" data-scroll-target="#retro-enc-dec-texttext-text-generation"><code>RETRO</code>: enc-dec text|text, text generation</a></li>
  <li><a href="#flamingo-enc-dec-imagetext-text-generation" id="toc-flamingo-enc-dec-imagetext-text-generation" class="nav-link" data-scroll-target="#flamingo-enc-dec-imagetext-text-generation"><code>Flamingo</code>: enc-dec image|text, text generation</a></li>
  <li><a href="#latent-diffusion-enc-dec-imagetext-image-generation" id="toc-latent-diffusion-enc-dec-imagetext-image-generation" class="nav-link" data-scroll-target="#latent-diffusion-enc-dec-imagetext-image-generation"><code>latent diffusion</code>: enc-dec image|text, image generation</a></li>
  <li><a href="#muse-imagetext-image-generation" id="toc-muse-imagetext-image-generation" class="nav-link" data-scroll-target="#muse-imagetext-image-generation"><code>Muse</code>: image|text, image generation</a></li>
  <li><a href="#speecht5-enc-dec-audiotext-text---audio-generation" id="toc-speecht5-enc-dec-audiotext-text---audio-generation" class="nav-link" data-scroll-target="#speecht5-enc-dec-audiotext-text---audio-generation"><code>SpeechT5</code>: enc-dec audio|text, text &lt;-&gt; audio generation</a></li>
  <li><a href="#gato-decoder-only-all2all" id="toc-gato-decoder-only-all2all" class="nav-link" data-scroll-target="#gato-decoder-only-all2all"><code>Gato</code>: decoder only all2all</a></li>
  </ul></li>
  <li><a href="#back-to-bottleneck-mid-fusion" id="toc-back-to-bottleneck-mid-fusion" class="nav-link" data-scroll-target="#back-to-bottleneck-mid-fusion">Back to bottleneck mid fusion</a>
  <ul class="collapse">
  <li><a href="#architecture" id="toc-architecture" class="nav-link" data-scroll-target="#architecture">architecture</a></li>
  <li><a href="#modality" id="toc-modality" class="nav-link" data-scroll-target="#modality">modality</a></li>
  </ul></li>
  <li><a href="#outro" id="toc-outro" class="nav-link" data-scroll-target="#outro">Outro</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/lukaemon/lukaemon.github.io/blob/main/posts/archive/from-text-to-multimodal/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="cover.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption">midjourney: fusion of multimodal sensory information –ar 16:9</figcaption>
</figure>
</div>
<p>Tracking the evolution trajectory from <span class="citation" data-cites="vaswaniAttentionAllYou2017">Vaswani et al. (<a href="#ref-vaswaniAttentionAllYou2017" role="doc-biblioref">2017</a>)</span> to cutting edge multimodal model <span class="citation" data-cites="nagraniAttentionBottlenecksMultimodal2022">(<a href="#ref-nagraniAttentionBottlenecksMultimodal2022" role="doc-biblioref">Nagrani et al. 2022</a>)</span> is an interesting journey. I’m mostly interested in multimodal fusion, meaning how to fuse information from different sensory modalities. A juxtaposition of 2 evolution tracks speaks volumes.</p>
<ol type="1">
<li>Text: 2017 OG encoder-decoder &gt; hyperscaled decoder</li>
<li>Multimodal: encoder-decoder and early fusion decoder &gt; bottleneck mid fusion</li>
</ol>
<p>To better appreciate the juxtaposition, a compare and contrast between the starting point and the cutting edge would be helpful.</p>
<section id="attention-is-all-you-need" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="attention-is-all-you-need">Attention is all you need</h2>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="attn_f1.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="vaswaniAttentionAllYou2017">Vaswani et al. (<a href="#ref-vaswaniAttentionAllYou2017" role="doc-biblioref">2017</a>)</span></figcaption>
</figure>
</div>
<p>Staring at this seminal encoder decoder architecture long enough makes me realize few things:</p>
<ul>
<li>Attention is applied to a set, not strictly on sequence. Very general and versatile inductive bias.</li>
<li>Sequence is just the result of information serialization. Text is natural sequence. Image tokens could be rasterized. Most of seq2seq magics are actually set2set plus optional positional information, such add-on info could be of many kinds.</li>
<li>The whole encoder stack plus the cross attention is an adapter module <span class="citation" data-cites="pfeifferModularDeepLearning2023">(<a href="#ref-pfeifferModularDeepLearning2023" role="doc-biblioref">Pfeiffer et al. 2023</a>)</span> to condition an autoregressive generative decoder stack.</li>
<li>The generative model doesn’t have to be autoregressive, or about text. The gold is multimodal representation learning. Generation is just one task.</li>
<li>The power of transformers applies to multimodal fusion. Text is just the beginning.</li>
</ul>
</section>
<section id="attention-bottleneck" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="attention-bottleneck">Attention bottleneck</h2>
<p>Inspired by this great presentation<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> from <a href="https://twitter.com/drjwrae?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">Jack Rae</a>, I realize that different form of bottleneck, ex: global workspace theory <span class="citation" data-cites="baarsGlobalWorkspaceTheory2017">(<a href="#ref-baarsGlobalWorkspaceTheory2017" role="doc-biblioref">Baars 2017</a>)</span>, could be interpreted as different expression of <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam’s razor</a>, which has deep root in information theory that regards effective compression as general intelligence.</p>
<div class="no-row-height column-margin column-container"><p><sup>1</sup>&nbsp;<a href="https://www.youtube.com/watch?v=dO4TPJkeaaU&amp;list=PLSrTvUm384I9PV10koj_cqit9OfbJXEkq&amp;index=79">Compression for AGI - Jack Rae</a></p></div><p>Copy and paste is just brute force. Information bottleneck is suffering that builds intelligence.</p>
<p><span class="citation" data-cites="nagraniAttentionBottlenecksMultimodal2022">Nagrani et al. (<a href="#ref-nagraniAttentionBottlenecksMultimodal2022" role="doc-biblioref">2022</a>)</span> introduces <code>bottleneck mid fusion</code>. I see it as successor to Gato’s <span class="citation" data-cites="reedGeneralistAgent2022">(<a href="#ref-reedGeneralistAgent2022" role="doc-biblioref">Reed et al. 2022</a>)</span> all you can eat style <code>early fusion</code>. This is a beautiful implementation of using bottleneck to foster learning intelligence.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="attn_bottleneck_f1.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="nagraniAttentionBottlenecksMultimodal2022">Nagrani et al. (<a href="#ref-nagraniAttentionBottlenecksMultimodal2022" role="doc-biblioref">2022</a>)</span></figcaption>
</figure>
</div>
<p>The beauty of <code>bottleneck mid fusion</code> comes from few realizations about multimodal learning:</p>
<blockquote class="blockquote">
<ol type="1">
<li>variations in learning dynamics between modalities</li>
<li>different <code>noise topologies</code>, with some modality streams containing more information for the task at hand than others</li>
<li>specialized input representations.</li>
</ol>
<p>–<span class="citation" data-cites="nagraniAttentionBottlenecksMultimodal2022">Nagrani et al. (<a href="#ref-nagraniAttentionBottlenecksMultimodal2022" role="doc-biblioref">2022</a>)</span></p>
</blockquote>
<p>Resonate pretty well with <span class="citation" data-cites="rombachHighResolutionImageSynthesis2022">Rombach et al. (<a href="#ref-rombachHighResolutionImageSynthesis2022" role="doc-biblioref">2022</a>)</span></p>
<blockquote class="blockquote">
<p>Most bits of a digital image correspond to imperceptible details. While DMs allow to suppress this semantically meaningless information by minimizing the responsible loss term, gradients (during training) and the neural network backbone (training and inference) still need to be evaluated on all pixels, leading to superfluous computations and unnecessarily expensive optimization and inference. We propose latent diffusion models (LDMs) as an effective generative model and a separate mild compression stage that only eliminates imperceptible details.</p>
</blockquote>
<p>You can see that <code>noise topology</code> exemplified with image. Bits are not equal to information, and human civilization builds semantics only on the subset of all information. Multimodal learning has to deal with different noise topology among different modalities. That’s why early fusion could work, but it would be far from <code>Pareto frontier</code>.</p>
<p><code>bottleneck mid fusion</code> is making <code>Pareto improvement</code> by doing mid fusion with information bottleneck. Mid fusion affords different modality independent computation to weed out noises. The context window of bottleneck token is global workspace, on which joint representation could be learned. The hope is quality of joint representation could be foster by imposed limitation.</p>
<p>With basic understandings about the baseline and cutting edge, moving on to juxtaposition.</p>
</section>
<section id="text-2017-og-encoder-decoder-hyperscaled-decoder" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="text-2017-og-encoder-decoder-hyperscaled-decoder">Text: 2017 OG encoder-decoder &gt; hyperscaled decoder</h2>
<p>Text2text treat input text as a modality. Info of input text is fused to text generative model to decide what to generate next. Encoder decoder architecture is basically a forced sparsity that implements a form of mid fusion and information bottleneck.</p>
<p>The whole encoder stack is isolated representation learning of input text. Regardless how many input tokens, as long as they are within context window, the output is a learned representation, a vector in latent space. Text to vector is a huge bottleneck. Useful information are encoded in the topological relationships in such latent space.</p>
<p>For example, the T5 model takes input text and transforms it into a learned representation for the decoder stack. This transformation involves mapping a <code>(number_of_token, d_model)</code> tensor to a <code>(d_model)</code> tensor where <code>d_model</code> is 768 according to the model’s configuration<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. The input goes through several transformer layers before producing this vector.</p>
<div class="no-row-height column-margin column-container"><p><sup>2</sup>&nbsp;<a href="https://huggingface.co/t5-base/blob/main/config.json#L7">config.json</a></p></div><p>The model’s bottleneck is in the encoder stack, where the input text is transformed into a 768-dimensional latent space. Each input text is represented as a unique vector in this space, and the model’s intelligence depends on the relationships between these vectors, particularly for input texts that the model has not seen before.</p>
<p>Representation is later used in cross-attention to contribute relevant information to the text generation stack. The relevancy is conditioned on generated text and the relevant info added to the output of masked attention</p>
<blockquote class="blockquote">
<p>All modular computation functions can be reduced to function composition: the output of the function <span class="math inline">\(f_{\theta_i}\)</span> of a model is added to a new term that depends on a learned function <span class="math inline">\(f_{\varphi}\)</span>: <span class="math inline">\(f'_i(x) = f_{\theta}(x) + f_{\varphi_i}(x)\)</span>.</p>
<p>–<span class="citation" data-cites="pfeifferModularDeepLearning2023">Pfeiffer et al. (<a href="#ref-pfeifferModularDeepLearning2023" role="doc-biblioref">2023</a>)</span></p>
</blockquote>
<p>You can see masked attention + cross attention is that addition. The purpose of the whole encoder stack and cross attention is to infuse input info to change how text generative function works. This is what adapters do.</p>
<p>Why don’t we have 100b+ T5, but many 100b+ decoder only LLM?</p>
<p>Text only modality doesn’t deal with <code>different noise topology</code>. It has one learning dynamics with coherent input representation. Forced mid fusion is unnecessary engineering. That’s why independent encoder is redundant especially during the push of hyperscaling to squeeze out the last few bits.</p>
<blockquote class="blockquote">
<p>…there are benefits to squeezing as much performance as possible out of large generative image models, as significant semantic information may lie in the <code>last few bits</code></p>
<p>–<span class="citation" data-cites="henighanScalingLawsAutoregressive2020">Henighan et al. (<a href="#ref-henighanScalingLawsAutoregressive2020" role="doc-biblioref">2020</a>)</span></p>
</blockquote>
<p>Small model (&lt;20b) is not as powerful as 100b+ model. Forced sparsity could be useful <code>inductive bias</code> to boost performance to a point. That’s why T5 works great for finetuning <span class="citation" data-cites="wangWhatLanguageModel2022a longpreFlanCollectionDesigning2023">(<a href="#ref-wangWhatLanguageModel2022a" role="doc-biblioref">Wang et al. 2022</a>; <a href="#ref-longpreFlanCollectionDesigning2023" role="doc-biblioref">Longpre et al. 2023</a>)</span>. It’s meant to be specialized. However, when the goal is foundation model that saturates one modality as much as possible, scale is one working formula and the raw performance is self-evident. Big models are more sample efficient, learn faster and compress better.</p>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="llama.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="touvronLLaMAOpenEfficient">Touvron et al. (<a href="#ref-touvronLLaMAOpenEfficient" role="doc-biblioref">2023</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="multimodal-encoder-decoder-and-early-fusion-decoder" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="multimodal-encoder-decoder-and-early-fusion-decoder">Multimodal: encoder-decoder and early fusion decoder</h2>
<p>It would be easier to see the basic pattern with few examples.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># patterns</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>fused_info <span class="op">=</span> fuse(modality_a, modality_b)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> task(fused_info)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Task itself is not that important. The point is learnable intelligence to get useful multimodal representation. Task is used to gauge whether the learned representation is useful.</p>
<section id="retro-enc-dec-texttext-text-generation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="retro-enc-dec-texttext-text-generation"><code>RETRO</code>: enc-dec text|text, text generation</h3>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="RETRO.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="borgeaudImprovingLanguageModels2022">Borgeaud et al. (<a href="#ref-borgeaudImprovingLanguageModels2022" role="doc-biblioref">2022</a>)</span></figcaption>
</figure>
</div>
<p>External memory as modality to power traditional text generation task. Retrieval as first class citizen not after thought as in-context retrieval <span class="citation" data-cites="mialonAugmentedLanguageModels2023">(<a href="#ref-mialonAugmentedLanguageModels2023" role="doc-biblioref">Mialon et al. 2023</a>)</span>.</p>
</section>
<section id="flamingo-enc-dec-imagetext-text-generation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="flamingo-enc-dec-imagetext-text-generation"><code>Flamingo</code>: enc-dec image|text, text generation</h3>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="Flamingo.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="alayracFlamingoVisualLanguage2022">Alayrac et al. (<a href="#ref-alayracFlamingoVisualLanguage2022" role="doc-biblioref">2022</a>)</span></figcaption>
</figure>
</div>
<p>Interleaving image and text. VQA is the logical next step of <code>ChatGPT</code>.</p>
</section>
<section id="latent-diffusion-enc-dec-imagetext-image-generation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="latent-diffusion-enc-dec-imagetext-image-generation"><code>latent diffusion</code>: enc-dec image|text, image generation</h3>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="latent_diffusion_f3.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="rombachHighResolutionImageSynthesis2022">Rombach et al. (<a href="#ref-rombachHighResolutionImageSynthesis2022" role="doc-biblioref">2022</a>)</span></figcaption>
</figure>
</div>
<p>Conditioned diffusion for image generation.</p>
</section>
<section id="muse-imagetext-image-generation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="muse-imagetext-image-generation"><code>Muse</code>: image|text, image generation</h3>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="muse.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="changMuseTextToImageGeneration2023">Chang et al. (<a href="#ref-changMuseTextToImageGeneration2023" role="doc-biblioref">2023</a>)</span></figcaption>
</figure>
</div>
<p>Like 2 layer enc-dec, diffusion free image generation. Very cool.</p>
</section>
<section id="speecht5-enc-dec-audiotext-text---audio-generation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="speecht5-enc-dec-audiotext-text---audio-generation"><code>SpeechT5</code>: enc-dec audio|text, text &lt;-&gt; audio generation</h3>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="speecht5_f2.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="aoSpeechT5UnifiedModalEncoderDecoder2022">Ao et al. (<a href="#ref-aoSpeechT5UnifiedModalEncoderDecoder2022" role="doc-biblioref">2022</a>)</span></figcaption>
</figure>
</div>
<p>Pre-post processing modules are adapters. Close to the idea:</p>
<blockquote class="blockquote">
<p>… image representations from vision models can be transferred as continuous prompts to frozen LMs by training only a single linear projection.</p>
<p>–<span class="citation" data-cites="merulloLinearlyMappingImage2022">Merullo et al. (<a href="#ref-merulloLinearlyMappingImage2022" role="doc-biblioref">2022</a>)</span></p>
</blockquote>
</section>
<section id="gato-decoder-only-all2all" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="gato-decoder-only-all2all"><code>Gato</code>: decoder only all2all</h3>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="Gato.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="reedGeneralistAgent2022">Reed et al. (<a href="#ref-reedGeneralistAgent2022" role="doc-biblioref">2022</a>)</span></figcaption>
</figure>
</div>
<p>A 1.2B parameter decoder-only transformer with 24 layers, an embedding size of 2048, and a post-attention feedforward hidden size of 8196.</p>
<p>The major contributions are multimodal tokenization, embedding and training objectives. Pretty sure DeepMind didn’t expect naive <code>early fusion</code> decoder to be the final answer of multimodal learning.</p>
<p>Or it’s that easy… Just few missing pieces to deal with such as limited context window, quadruple computation of self-attention and so on. Given recent light speed of AI progress, I don’t think Gato is the answer but I’m not so sure anymore LoL.</p>
<hr>
<p>All above are interesting. The holy grail is to find a scaling friendly formula for multimodal learning just like GPT3 did to text. Scaling friendly in terms of:</p>
<ul>
<li>Simple, easy, parallelizable architecture.</li>
<li>BIG data.</li>
<li>Self supervised objective function.</li>
</ul>
<p>Current multimodal models are far from settling down on the architecture. Image caption dataset is great but one simply can’t expect to find enough <code>(x, natural language)</code> supervision for every modality. Too many hard to describe intelligence going on daily. Try to explain the internal state of playing piano or the touch of a perfect golf swing. There would always be some modalities that have small cross-modal latent representation. Language is useful but not the universal multimodal substrate.</p>
<p>I just don’t see current mutation of multimodal enc-dec or simple multimodal decoder architecture are there yet. To be fair, some are already very useful.</p>
</section>
</section>
<section id="back-to-bottleneck-mid-fusion" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="back-to-bottleneck-mid-fusion">Back to bottleneck mid fusion</h2>
<p>Both architecture and choice of modalities are interesting.</p>
<section id="architecture" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="architecture">architecture</h3>
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="attn_bottleneck_f2.png" class="img-fluid figure-img"></p>
<figcaption class="margin-caption"><span class="citation" data-cites="nagraniAttentionBottlenecksMultimodal2022">Nagrani et al. (<a href="#ref-nagraniAttentionBottlenecksMultimodal2022" role="doc-biblioref">2022</a>)</span></figcaption>
</figure>
</div>
<p>Intuitively, late fusion and early fusion can’t be optimal. Remember we want to scale like crazy. Manually engineered enc-dec is not optimal as well. The paper use few <code>bottleneck token</code> and <code>bottleneck transformer</code> to replace the whole encoder plus cross attention to decoder. The size of bottleneck token could be a hyperparameter to control the size of cross-modal latent space as in <span class="citation" data-cites="aoSpeechT5UnifiedModalEncoderDecoder2022">Ao et al. (<a href="#ref-aoSpeechT5UnifiedModalEncoderDecoder2022" role="doc-biblioref">2022</a>)</span>.</p>
<p>The architecture is simple and elegant.</p>
</section>
<section id="modality" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="modality">modality</h3>
<p>Given the following context:</p>
<ul>
<li>Text is done. Mobile phone kind of done that innovations are still expected but the field is a perfect competition market. The speed, price and quality of <code>ChatGPT API</code> is crazy.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></li>
<li><code>(text, image) -&gt; image</code> is in the instruction finetuning stage <span class="citation" data-cites="zhangAddingConditionalControl2023">(<a href="#ref-zhangAddingConditionalControl2023" role="doc-biblioref">Zhang and Agrawala 2023</a>)</span>.<br>
</li>
<li><code>(text, image) -&gt; text</code> is improving super fast <span class="citation" data-cites="kohGroundingLanguageModels2023 liBLIP2BootstrappingLanguageImage2023 huangLanguageNotAll2023">(<a href="#ref-kohGroundingLanguageModels2023" role="doc-biblioref">Koh, Salakhutdinov, and Fried 2023</a>; <a href="#ref-liBLIP2BootstrappingLanguageImage2023" role="doc-biblioref">Li et al. 2023</a>; <a href="#ref-huangLanguageNotAll2023" role="doc-biblioref">Huang et al. 2023</a>)</span></li>
</ul>
<div class="no-row-height column-margin column-container"><p><sup>3</sup>&nbsp;<a href="https://openai.com/blog/introducing-chatgpt-and-whisper-apis">Introducing ChatGPT and Whisper APIs</a></p></div><p><code>(image, audio) -&gt; x</code> is a good proxy to think about how and what &lt;12 month baby could learn. It is right at the multimodal research frontier. I see this as the last frontier before embodied learning and some even argue body is not necessary for high intelligence. Reviewing my daily activity, input and output, the idea of body is not necessary for average level intelligence finds support lol.</p>
<p>Video is abundant. Learning dynamics and noise topology of image and audio are real challenges. Yes we could use natural language to bridge them but that is boring and even it works to an extend, better solution is expected.</p>
</section>
</section>
<section id="outro" class="level2">
<h2 class="anchored" data-anchor-id="outro">Outro</h2>
<p>I’m facing 2 problems to further exploring this track of research:</p>
<ol type="1">
<li>Weak information theory understanding.</li>
<li>Weak engineering capabilities to carry on experiments effectively.</li>
</ol>
<p><span class="citation" data-cites="aghajanyanScalingLawsGenerative2023">Aghajanyan et al. (<a href="#ref-aghajanyanScalingLawsGenerative2023" role="doc-biblioref">2023</a>)</span> could help me get oriented to information theory wrt multimodal research but the theoretical gap is too big to bridge with just one paper. Time to read textbooks.</p>
<p>End of procrastination. Back to code and read.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-aghajanyanScalingLawsGenerative2023" class="csl-entry" role="listitem">
Aghajanyan, Armen, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. 2023. <span>“Scaling <span>Laws</span> for <span>Generative Mixed-Modal Language Models</span>.”</span> January 9, 2023. <a href="http://arxiv.org/abs/2301.03728">http://arxiv.org/abs/2301.03728</a>.
</div>
<div id="ref-alayracFlamingoVisualLanguage2022" class="csl-entry" role="listitem">
Alayrac, Jean-Baptiste, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, et al. 2022. <span>“🦩 <span>Flamingo</span>: A <span>Visual Language Model</span> for <span>Few-Shot Learning</span>,”</span> April, 66.
</div>
<div id="ref-aoSpeechT5UnifiedModalEncoderDecoder2022" class="csl-entry" role="listitem">
Ao, Junyi, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, et al. 2022. <span>“<span>SpeechT5</span>: <span>Unified-Modal Encoder-Decoder Pre-Training</span> for <span>Spoken Language Processing</span>.”</span> May 24, 2022. <a href="http://arxiv.org/abs/2110.07205">http://arxiv.org/abs/2110.07205</a>.
</div>
<div id="ref-baarsGlobalWorkspaceTheory2017" class="csl-entry" role="listitem">
Baars, Bernard J. 2017. <span>“The <span>Global Workspace Theory</span> of <span>Consciousness</span>.”</span> In <em>The <span>Blackwell Companion</span> to <span>Consciousness</span></em>, 227–42. <span>John Wiley &amp; Sons, Ltd</span>. <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119132363.ch16">https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119132363.ch16</a>.
</div>
<div id="ref-borgeaudImprovingLanguageModels2022" class="csl-entry" role="listitem">
Borgeaud, Sebastian, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, et al. 2022. <span>“Improving Language Models by Retrieving from Trillions of Tokens.”</span> February 7, 2022. <a href="http://arxiv.org/abs/2112.04426">http://arxiv.org/abs/2112.04426</a>.
</div>
<div id="ref-changMuseTextToImageGeneration2023" class="csl-entry" role="listitem">
Chang, Huiwen, Han Zhang, Jarred Barber, A. J. Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, et al. 2023. <span>“Muse: <span>Text-To-Image Generation</span> via <span>Masked Generative Transformers</span>.”</span> January 2, 2023. <a href="http://arxiv.org/abs/2301.00704">http://arxiv.org/abs/2301.00704</a>.
</div>
<div id="ref-henighanScalingLawsAutoregressive2020" class="csl-entry" role="listitem">
Henighan, Tom, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, et al. 2020. <span>“Scaling <span>Laws</span> for <span>Autoregressive Generative Modeling</span>.”</span> November 5, 2020. <a href="http://arxiv.org/abs/2010.14701">http://arxiv.org/abs/2010.14701</a>.
</div>
<div id="ref-huangLanguageNotAll2023" class="csl-entry" role="listitem">
Huang, Shaohan, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, et al. 2023. <span>“Language <span>Is Not All You Need</span>: <span>Aligning Perception</span> with <span>Language Models</span>.”</span> March 1, 2023. <a href="http://arxiv.org/abs/2302.14045">http://arxiv.org/abs/2302.14045</a>.
</div>
<div id="ref-kohGroundingLanguageModels2023" class="csl-entry" role="listitem">
Koh, Jing Yu, Ruslan Salakhutdinov, and Daniel Fried. 2023. <span>“Grounding <span>Language Models</span> to <span>Images</span> for <span>Multimodal Generation</span>.”</span> January 31, 2023. <a href="http://arxiv.org/abs/2301.13823">http://arxiv.org/abs/2301.13823</a>.
</div>
<div id="ref-liBLIP2BootstrappingLanguageImage2023" class="csl-entry" role="listitem">
Li, Junnan, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. <span>“<span>BLIP-2</span>: <span class="nocase">Bootstrapping Language-Image Pre-training</span> with <span>Frozen Image Encoders</span> and <span>Large Language Models</span>.”</span> January 29, 2023. <a href="http://arxiv.org/abs/2301.12597">http://arxiv.org/abs/2301.12597</a>.
</div>
<div id="ref-longpreFlanCollectionDesigning2023" class="csl-entry" role="listitem">
Longpre, Shayne, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, et al. 2023. <span>“The <span>Flan Collection</span>: <span>Designing Data</span> and <span>Methods</span> for <span>Effective Instruction Tuning</span>.”</span> January 31, 2023. <a href="http://arxiv.org/abs/2301.13688">http://arxiv.org/abs/2301.13688</a>.
</div>
<div id="ref-merulloLinearlyMappingImage2022" class="csl-entry" role="listitem">
Merullo, Jack, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. 2022. <span>“Linearly <span>Mapping</span> from <span>Image</span> to <span>Text Space</span>.”</span> September 29, 2022. <a href="http://arxiv.org/abs/2209.15162">http://arxiv.org/abs/2209.15162</a>.
</div>
<div id="ref-mialonAugmentedLanguageModels2023" class="csl-entry" role="listitem">
Mialon, Grégoire, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, et al. 2023. <span>“Augmented <span>Language Models</span>: A <span>Survey</span>.”</span> February 15, 2023. <a href="http://arxiv.org/abs/2302.07842">http://arxiv.org/abs/2302.07842</a>.
</div>
<div id="ref-nagraniAttentionBottlenecksMultimodal2022" class="csl-entry" role="listitem">
Nagrani, Arsha, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. 2022. <span>“Attention <span>Bottlenecks</span> for <span>Multimodal Fusion</span>.”</span> November 30, 2022. <a href="http://arxiv.org/abs/2107.00135">http://arxiv.org/abs/2107.00135</a>.
</div>
<div id="ref-pfeifferModularDeepLearning2023" class="csl-entry" role="listitem">
Pfeiffer, Jonas, Sebastian Ruder, Ivan Vulić, and Edoardo Maria Ponti. 2023. <span>“Modular <span>Deep Learning</span>.”</span> February 22, 2023. <a href="http://arxiv.org/abs/2302.11529">http://arxiv.org/abs/2302.11529</a>.
</div>
<div id="ref-reedGeneralistAgent2022" class="csl-entry" role="listitem">
Reed, Scott, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, et al. 2022. <span>“A <span>Generalist Agent</span>.”</span> May 19, 2022. <a href="http://arxiv.org/abs/2205.06175">http://arxiv.org/abs/2205.06175</a>.
</div>
<div id="ref-rombachHighResolutionImageSynthesis2022" class="csl-entry" role="listitem">
Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. <span>“High-<span>Resolution Image Synthesis</span> with <span>Latent Diffusion Models</span>.”</span> April 13, 2022. <a href="http://arxiv.org/abs/2112.10752">http://arxiv.org/abs/2112.10752</a>.
</div>
<div id="ref-touvronLLaMAOpenEfficient" class="csl-entry" role="listitem">
Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, et al. 2023. <span>“<span>LLaMA</span>: <span>Open</span> and <span>Efficient Foundation Language Models</span>,”</span> February.
</div>
<div id="ref-vaswaniAttentionAllYou2017" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention <span>Is All You Need</span>.”</span> <a href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a>.
</div>
<div id="ref-wangWhatLanguageModel2022a" class="csl-entry" role="listitem">
Wang, Thomas, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffel. 2022. <span>“What <span>Language Model Architecture</span> and <span>Pretraining Objective Work Best</span> for <span>Zero-Shot Generalization</span>?”</span> April 12, 2022. <a href="http://arxiv.org/abs/2204.05832">http://arxiv.org/abs/2204.05832</a>.
</div>
<div id="ref-zhangAddingConditionalControl2023" class="csl-entry" role="listitem">
Zhang, Lvmin, and Maneesh Agrawala. 2023. <span>“Adding <span>Conditional Control</span> to <span class="nocase">Text-to-Image Diffusion Models</span>.”</span> February 10, 2023. <a href="http://arxiv.org/abs/2302.05543">http://arxiv.org/abs/2302.05543</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{shen2023,
  author = {Shen, Lucas},
  title = {From Vanilla Text Encoder Decoder to Multimodal Mid Fusion
    Attention Bottleneck},
  date = {2023-03-06},
  url = {https://lukaemon.github.io/posts/archive/from-text-to-multimodal},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-shen2023" class="csl-entry quarto-appendix-citeas" role="listitem">
Shen, Lucas. 2023. <span>“From Vanilla Text Encoder Decoder to
Multimodal Mid Fusion Attention Bottleneck.”</span> March 6, 2023. <a href="https://lukaemon.github.io/posts/archive/from-text-to-multimodal">https://lukaemon.github.io/posts/archive/from-text-to-multimodal</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/lukaemon/lukaemon.github.io/blob/main/posts/archive/from-text-to-multimodal/index.qmd" class="toc-action"><i class="bi bi-github"></i>View source</a></li></ul></div></div></div></footer></body></html>