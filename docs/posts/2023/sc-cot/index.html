<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lucas Shen">
<meta name="dcterms.date" content="2023-02-05">
<meta name="description" content="Lesson learned from CoT and SC.">

<title>Lucas Shen - Self-consistency and chain of thought</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../asset/favicon.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<meta property="og:title" content="Lucas Shen - Self-consistency and chain of thought">
<meta property="og:description" content="Lesson learned from CoT and SC.">
<meta property="og:image" content="https://lukaemon.github.io/posts/2023/sc-cot/sc.png">
<meta property="og:site-name" content="Lucas Shen">
<meta property="og:image:height" content="1009">
<meta property="og:image:width" content="1687">
<meta name="twitter:title" content="Lucas Shen - Self-consistency and chain of thought">
<meta name="twitter:description" content="Lesson learned from CoT and SC.">
<meta name="twitter:image" content="https://lukaemon.github.io/posts/2023/sc-cot/sc.png">
<meta name="twitter:image-height" content="1009">
<meta name="twitter:image-width" content="1687">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Lucas Shen</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lukaemon"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/luka_emon"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../index.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Self-consistency and chain of thought</h1>
                  <div>
        <div class="description">
          Lesson learned from CoT and SC.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">rationale engineering</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Lucas Shen </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 5, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#reasoning" id="toc-reasoning" class="nav-link active" data-scroll-target="#reasoning">Reasoning</a></li>
  <li><a href="#retrieval" id="toc-retrieval" class="nav-link" data-scroll-target="#retrieval">Retrieval</a></li>
  <li><a href="#multimodal" id="toc-multimodal" class="nav-link" data-scroll-target="#multimodal">Multimodal</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/lukaemon/lukaemon.github.io/blob/main/posts/2023/sc-cot/index.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">




<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="sc.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption margin-caption"><span class="citation" data-cites="wangSelfConsistencyImprovesChain2022a">Wang et al. (<a href="#ref-wangSelfConsistencyImprovesChain2022a" role="doc-biblioref">2022</a>)</span></figcaption><p></p>
</figure>
</div>
<section id="reasoning" class="level2">
<h2 class="anchored" data-anchor-id="reasoning">Reasoning</h2>
<blockquote class="blockquote">
<p>Scale has a reduced benefit for tasks in the Maths, Logical Reasoning, and Common Sense categories. Our results suggest that for certain flavours of mathematical or logical reasoning tasks, it is unlikely that scale alone will lead to performance breakthroughs. ‚Äì<span class="citation" data-cites="raeScalingLanguageModels2022">(<a href="#ref-raeScalingLanguageModels2022" role="doc-biblioref">Rae et al. 2022</a>)</span></p>
</blockquote>
<p>It points out flat scaling curve of few task categories. Since then, google has been very creative to push the frontier with CoT <span class="citation" data-cites="weiChainThoughtPrompting2022">(<a href="#ref-weiChainThoughtPrompting2022" role="doc-biblioref">Wei et al. 2022</a>)</span>, SC <span class="citation" data-cites="wangSelfConsistencyImprovesChain2022a">(<a href="#ref-wangSelfConsistencyImprovesChain2022a" role="doc-biblioref">Wang et al. 2022</a>)</span> and least to most <span class="citation" data-cites="zhouLeasttoMostPromptingEnables2022">(<a href="#ref-zhouLeasttoMostPromptingEnables2022" role="doc-biblioref">Zhou et al. 2022</a>)</span>. CoT is the most exciting method to scale computation on tasks since few-shot in-context learning.</p>
<p>Informal reasoning would be solved. DeepMind and OpenAI are all into solving formal reasoning, the last frontier wrt reasoning if AI could get logic and math right.</p>
<p>Codex family model is the first step on solving formal reasoning. In SC and BBH <span class="citation" data-cites="suzgunChallengingBIGBenchTasks2022">(<a href="#ref-suzgunChallengingBIGBenchTasks2022" role="doc-biblioref">Suzgun et al. 2022</a>)</span>, code-davinci-002 performs better than InstructGPT families on reasoning tasks. DeepMind even dives into GNN to explore architecture other than transformer. Reasoning in general would be solved as a modality in near future. It may require a specialized model, but would ultimately be fused into general LLM like image, audio and the like.</p>
<blockquote class="blockquote">
<p>The approach to the irreducible loss does not necessarily indicate diminishing returns for representation quality or semantic content as significant semantic information may lie in the last few bits. ‚Äì<span class="citation" data-cites="henighanScalingLawsAutoregressive2020">(<a href="#ref-henighanScalingLawsAutoregressive2020" role="doc-biblioref">Henighan et al. 2020</a>)</span></p>
</blockquote>
<p>To get natural language understanding right, scale is necessary. This also explains why CoT only works with scale. Small model makes too many semantic mistakes that render scaling computation with CoT worthless. SC could cancel out mistakes by majority vote to improve performance for model of all size but the increased computational cost far out weight possible gain for small model. Self-ensemble weak reasoner is a waste of resource.</p>
</section>
<section id="retrieval" class="level2">
<h2 class="anchored" data-anchor-id="retrieval">Retrieval</h2>
<p>Scale may not be the most effective method to solve world knowledge problem. 1T param model may get the last few bit of semantics but it won‚Äôt get the facts 100% right. That‚Äôs why retrieval is necessary. One could treat external knowledge database as one modality and figure out how to fuse it with general LLM.</p>
<p>Think about how existing multimodal model fuses modalities, ex: Dall-E <span class="citation" data-cites="rameshHierarchicalTextConditionalImage2022">(<a href="#ref-rameshHierarchicalTextConditionalImage2022" role="doc-biblioref">Ramesh et al. 2022</a>)</span>, Diffusion <span class="citation" data-cites="rombachHighResolutionImageSynthesis2022">(<a href="#ref-rombachHighResolutionImageSynthesis2022" role="doc-biblioref">Rombach et al. 2022</a>)</span>, MusicLM <span class="citation" data-cites="agostinelliMusicLMGeneratingMusic2023">(<a href="#ref-agostinelliMusicLMGeneratingMusic2023" role="doc-biblioref">Agostinelli et al. 2023</a>)</span>. RETRO <span class="citation" data-cites="borgeaudImprovingLanguageModels2022">(<a href="#ref-borgeaudImprovingLanguageModels2022" role="doc-biblioref">Borgeaud et al. 2022</a>)</span> is a great example of treating external memory as modality and fuse it with general LM deeply. Of course it‚Äôs not plug and play but still a very interesting direction.</p>
<p>In-context retrieval dominates current research output because of light resource requirement. Its value is similar to prompt engineering: the most effective method to probe LLM to find new gains, but prompt engineering would never be the ultimate solution. It‚Äôs a tentative exploration process. Like instruction finetuning makes LLM to follow human instruction and do CoT in 0 shot, rather than few shot, RETRO like solution may render methods such as recitaiton <span class="citation" data-cites="sunRecitationAugmentedLanguageModels2022a">(<a href="#ref-sunRecitationAugmentedLanguageModels2022a" role="doc-biblioref">Sun et al. 2022</a>)</span> unnecessary. However, recitation to me is like SC for open ended text generation, which is one great first step into retrieval world by scaling computation on retrieval tasks, like CoT to rationale engineering.</p>
</section>
<section id="multimodal" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="multimodal">Multimodal</h2>
<div class="page-columns page-full"><p>500b+ dense LLM, 1T+ MoE, text-davinci-003 are great and useful but not enough. Have to find a way to fuse modalities. Small model like T5-11b, yes 11b is the new small lol, is still important for controlling latency and cost. Imagine doing 40 path SC on a 540b model per response for interactive UX. Not ideal. A good production example: Neeva<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;<a href="https://twitter.com/ramaswmysridhar/status/1621870491945533440?s=12&amp;t=nyAGas8S6bDKS1eLUw9I7Q">T5 for serving ChatGPT like search</a></p></li></div></div>
<p>Multimodal is on fire. One big end to end model may be enough, ex: Gato <span class="citation" data-cites="reedGeneralistAgent2022">(<a href="#ref-reedGeneralistAgent2022" role="doc-biblioref">Reed et al. 2022</a>)</span>. On the other hand, modular approach with glue architecture may work, ex: Flamingo <span class="citation" data-cites="alayracFlamingoVisualLanguage2022">(<a href="#ref-alayracFlamingoVisualLanguage2022" role="doc-biblioref">Alayrac et al. 2022</a>)</span> and RETRO. It‚Äôs great to be alive in this era of AI.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-agostinelliMusicLMGeneratingMusic2023" class="csl-entry" role="doc-biblioentry">
Agostinelli, Andrea, Timo I. Denk, Zal√°n Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, et al. 2023. <span>‚Äú<span>MusicLM</span>: <span>Generating Music From Text</span>.‚Äù</span> <span>arXiv</span>. <a href="http://arxiv.org/abs/2301.11325">http://arxiv.org/abs/2301.11325</a>.
</div>
<div id="ref-alayracFlamingoVisualLanguage2022" class="csl-entry" role="doc-biblioentry">
Alayrac, Jean-Baptiste, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, et al. 2022. <span>‚Äúü¶© <span>Flamingo</span>: A <span>Visual Language Model</span> for <span>Few-Shot Learning</span>,‚Äù</span> April, 66.
</div>
<div id="ref-borgeaudImprovingLanguageModels2022" class="csl-entry" role="doc-biblioentry">
Borgeaud, Sebastian, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, et al. 2022. <span>‚ÄúImproving Language Models by Retrieving from Trillions of Tokens.‚Äù</span> <span>arXiv</span>. <a href="http://arxiv.org/abs/2112.04426">http://arxiv.org/abs/2112.04426</a>.
</div>
<div id="ref-henighanScalingLawsAutoregressive2020" class="csl-entry" role="doc-biblioentry">
Henighan, Tom, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, et al. 2020. <span>‚ÄúScaling <span>Laws</span> for <span>Autoregressive Generative Modeling</span>.‚Äù</span> <span>arXiv</span>. <a href="http://arxiv.org/abs/2010.14701">http://arxiv.org/abs/2010.14701</a>.
</div>
<div id="ref-raeScalingLanguageModels2022" class="csl-entry" role="doc-biblioentry">
Rae, Jack W., Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, et al. 2022. <span>‚ÄúScaling <span>Language Models</span>: <span>Methods</span>, <span>Analysis</span> &amp; <span>Insights</span> from <span>Training Gopher</span>.‚Äù</span> <a href="http://arxiv.org/abs/2112.11446">http://arxiv.org/abs/2112.11446</a>.
</div>
<div id="ref-rameshHierarchicalTextConditionalImage2022" class="csl-entry" role="doc-biblioentry">
Ramesh, Aditya, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. <span>‚ÄúHierarchical <span>Text-Conditional Image Generation</span> with <span>CLIP Latents</span>,‚Äù</span> 26.
</div>
<div id="ref-reedGeneralistAgent2022" class="csl-entry" role="doc-biblioentry">
Reed, Scott, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, et al. 2022. <span>‚ÄúA <span>Generalist Agent</span>.‚Äù</span> <span>arXiv</span>. <a href="http://arxiv.org/abs/2205.06175">http://arxiv.org/abs/2205.06175</a>.
</div>
<div id="ref-rombachHighResolutionImageSynthesis2022" class="csl-entry" role="doc-biblioentry">
Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. 2022. <span>‚ÄúHigh-<span>Resolution Image Synthesis</span> with <span>Latent Diffusion Models</span>.‚Äù</span> <span>arXiv</span>. <a href="http://arxiv.org/abs/2112.10752">http://arxiv.org/abs/2112.10752</a>.
</div>
<div id="ref-sunRecitationAugmentedLanguageModels2022a" class="csl-entry" role="doc-biblioentry">
Sun, Zhiqing, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2022. <span>‚ÄúRecitation-<span>Augmented Language Models</span>.‚Äù</span> <span>arXiv</span>. <a href="http://arxiv.org/abs/2210.01296">http://arxiv.org/abs/2210.01296</a>.
</div>
<div id="ref-suzgunChallengingBIGBenchTasks2022" class="csl-entry" role="doc-biblioentry">
Suzgun, Mirac, Nathan Scales, Nathanael Sch√§rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, et al. 2022. <span>‚ÄúChallenging <span>BIG-Bench Tasks</span> and <span class="nocase">Whether Chain-of-Thought Can Solve Them</span>,‚Äù</span> October. <a href="https://arxiv.org/abs/2210.09261v1">https://arxiv.org/abs/2210.09261v1</a>.
</div>
<div id="ref-wangSelfConsistencyImprovesChain2022a" class="csl-entry" role="doc-biblioentry">
Wang, Xuezhi, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. <span>‚ÄúSelf-<span>Consistency Improves Chain</span> of <span>Thought Reasoning</span> in <span>Language Models</span>.‚Äù</span> <span>arXiv</span>. <a href="http://arxiv.org/abs/2203.11171">http://arxiv.org/abs/2203.11171</a>.
</div>
<div id="ref-weiChainThoughtPrompting2022" class="csl-entry" role="doc-biblioentry">
Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. <span>‚ÄúChain of <span>Thought Prompting Elicits Reasoning</span> in <span>Large Language Models</span>.‚Äù</span> <a href="https://arxiv.org/abs/2201.11903v5">https://arxiv.org/abs/2201.11903v5</a>.
</div>
<div id="ref-zhouLeasttoMostPromptingEnables2022" class="csl-entry" role="doc-biblioentry">
Zhou, Denny, Nathanael Sch√§rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, et al. 2022. <span>‚ÄúLeast-to-<span>Most Prompting Enables Complex Reasoning</span> in <span>Large Language Models</span>.‚Äù</span> <span>arXiv</span>. <a href="http://arxiv.org/abs/2205.10625">http://arxiv.org/abs/2205.10625</a>.
</div>
</div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{shen2023,
  author = {Lucas Shen},
  title = {Self-Consistency and Chain of Thought},
  date = {2023-02-05},
  url = {https://lukaemon.github.io/posts/2023/sc-cot},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-shen2023" class="csl-entry quarto-appendix-citeas" role="doc-biblioentry">
Lucas Shen. 2023. <span>‚ÄúSelf-Consistency and Chain of Thought.‚Äù</span>
February 5, 2023. <a href="https://lukaemon.github.io/posts/2023/sc-cot">https://lukaemon.github.io/posts/2023/sc-cot</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>