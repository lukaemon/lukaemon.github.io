{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Hello, world! Huggingface T5 finetuning\"\n",
    "description: How to finetune Flan-T5 with samsum dataset?\n",
    "date: \"2023-02-10\"\n",
    "categories: [tutorial]\n",
    "image: \"cover.png\"\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MJ: computer scientist coding to train AI model, studio ghibli --ar 16:9 --niji](cover.png)\n",
    "\n",
    "^[MidJourney implies the future belongs to children playing Scratch lol.]\n",
    "\n",
    "A learning note from reproducing this [amazing post by Philipp Schmid\n",
    "](https://www.philschmid.de/fine-tune-flan-t5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "import os\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "checkpoint = \"google/flan-t5-base\"\n",
    "dataset_name = \"samsum\"\n",
    "\n",
    "ft_output_dir = os.getenv(\"HF_FINETUNE_OUTPUT_DIR\")\n",
    "model_name = checkpoint.split(\"/\")[-1]\n",
    "hub_model_id = f\"{model_name}-{dataset_name}\"\n",
    "model_output_dir = os.path.join(ft_output_dir, hub_model_id)\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = hub_model_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "`samsum` is a conversation dataset. The goal is to summarize a conversation. Dataset is available on [Huggingface](https://huggingface.co/datasets/samsum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e2dd6627d6431f8867e0616b3ced63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(dataset_name)\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '13818513',\n",
       " 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n",
       " 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ds[\"train\"][0]\n",
    "example\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `max_length` analysis\n",
    "Investigate [truncation and padding](https://huggingface.co/docs/transformers/main/en/pad_truncation#padding-and-truncation) to get statistics on dialogue and summary token length.\n",
    "\n",
    "Outlier long input may cause out of memory error during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "model.parallelize()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dialogue  summary\n",
      "count   14732.0  14732.0\n",
      "mean      149.0     28.9\n",
      "std       110.7     15.1\n",
      "min         1.0      2.0\n",
      "25%        66.0     17.0\n",
      "50%       120.0     26.0\n",
      "75%       202.0     37.0\n",
      "max      1153.0     94.0\n"
     ]
    }
   ],
   "source": [
    "tk_dialogue = tokenizer(ds[\"train\"][\"dialogue\"])[\"input_ids\"]\n",
    "tk_summary = tokenizer(ds[\"train\"][\"summary\"])[\"input_ids\"]\n",
    "pd.set_option('display.float_format', lambda x: '%.1f' % x)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\"dialogue\": [len(d) for d in tk_dialogue], \"summary\": [len(s) for s in tk_summary]}\n",
    ")\n",
    "print(df.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first hunch is I shouldn't truncate the input. Just need to pad to the longest of the batch. \n",
    "The setting would be `tokenizer(batch_sentences, padding=True)`.  \n",
    "\n",
    "However, it seems that [truncation is inevitable in production](https://twitter.com/RamaswmySridhar/status/1621870502766858241). You need to find a balance and curb the long input outlier. \n",
    "\n",
    "For this dataset, 1153 max is not too crazy. \n",
    "\n",
    "### Padding experiments\n",
    "Let's experiment with different padding strategy and how it affects the following batching and training.\n",
    "\n",
    "First, do it without truncation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dialogue  summary\n",
      "count   14732.0  14732.0\n",
      "mean     1153.0     94.0\n",
      "std         0.0      0.0\n",
      "min      1153.0     94.0\n",
      "25%      1153.0     94.0\n",
      "50%      1153.0     94.0\n",
      "75%      1153.0     94.0\n",
      "max      1153.0     94.0\n"
     ]
    }
   ],
   "source": [
    "tk_dialogue = tokenizer(ds[\"train\"][\"dialogue\"], padding=True)[\"input_ids\"]\n",
    "tk_summary = tokenizer(ds[\"train\"][\"summary\"], padding=True)[\"input_ids\"]\n",
    "pd.set_option('display.float_format', lambda x: '%.1f' % x)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\"dialogue\": [len(d) for d in tk_dialogue], \"summary\": [len(s) for s in tk_summary]}\n",
    ")\n",
    "print(df.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected result. This is literally treating the whole training corpus as one full batch. All sequences are pad to the max length, 1153.  \n",
    "\n",
    "Try this idea with `batch_size = 8` in dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1842 1842\n",
      "482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1153, 389.02904564315355, 92)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, padding=True)\n",
    "dl = DataLoader(ds['train'].with_transform(lambda x: tokenizer(x['dialogue'])), batch_size=8, collate_fn=collator)\n",
    "\n",
    "\n",
    "tk_batched = np.array([batch['input_ids'].shape[-1] for batch in dl])\n",
    "\n",
    "print(len(tk_batched), len(dl))\n",
    "print(len(np.unique(tk_batched)))\n",
    "\n",
    "np.unique(tk_batched).max(), np.unique(tk_batched).mean(), np.unique(tk_batched).min()b\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1842 batches, with 482 unique length. This is fine for `pytorch` but would be brutal for jax jit since every change of input shape would [trigger jit recompilation](https://huggingface.co/docs/transformers/main/en/model_doc/t5#training).\n",
    "\n",
    "> If training on TPU, it is recommended to pad all examples of the dataset to the same length or make use of pad_to_multiple_of to have a small number of predefined bucket sizes to fit all examples in. Dynamically padding batches to the longest example is not recommended on TPU as it triggers a recompilation for every batch shape that is encountered during training thus significantly slowing down the training. only padding up to the longest example in a batch) leads to very slow training on TPU.\n",
    "\n",
    "The part of only padding to the longest leads to slow training applies to `pytorch` as well.\n",
    "\n",
    "Try `pad_to_multiple_of=8` to curb the variance of token length in batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1842 1842\n",
      "91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1160, 485.27472527472526, 96)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator = DataCollatorForSeq2Seq(tokenizer, padding=True, pad_to_multiple_of=8)\n",
    "dl = DataLoader(ds['train'].with_transform(lambda x: tokenizer(x['dialogue'])), batch_size=8, collate_fn=collator)\n",
    "\n",
    "\n",
    "tk_batched = np.array([batch['input_ids'].shape[-1] for batch in dl])\n",
    "\n",
    "print(len(tk_batched), len(dl))\n",
    "print(len(np.unique(tk_batched)))\n",
    "\n",
    "np.unique(tk_batched).max(), np.unique(tk_batched).mean(), np.unique(tk_batched).min()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1842 batches with 91 unique lengths, much better. \n",
    "\n",
    "### Truncation experiment\n",
    "How does `truncation=True` change anything? According to huggingface doc: `tokenizer(batch_sentences, padding=True, truncation=True)` has the same effect as `tokenizer(batch_sentences, padding=True)`, both padding to max sequence in batch. \n",
    "\n",
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1842 1842\n",
      "51\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(512, 311.52941176470586, 96)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator = DataCollatorForSeq2Seq(tokenizer, padding=True, pad_to_multiple_of=8)\n",
    "dl = DataLoader(ds['train'].with_transform(lambda x: tokenizer(x['dialogue'], truncation=True)), batch_size=8, collate_fn=collator)\n",
    "\n",
    "\n",
    "tk_batched = np.array([batch['input_ids'].shape[-1] for batch in dl])\n",
    "\n",
    "print(len(tk_batched), len(dl))\n",
    "print(len(np.unique(tk_batched)))\n",
    "\n",
    "np.unique(tk_batched).max(), np.unique(tk_batched).mean(), np.unique(tk_batched).min()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`truncation=True` in the tokenizer truncates the dialogue to 512 tokens, which is the max length of the T5. However, by default T5 should not have a set maximum length. This is imposed, artificial limitation by transformers library.\n",
    "\n",
    "Be careful to this behavior. Since unnoticed truncation means unnoticed loss input information during training. \n",
    "\n",
    "### Source implementation\n",
    "In [source ipynb](https://www.philschmid.de/fine-tune-flan-t5): \n",
    "```python\n",
    "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "\n",
    "def preprocess_function(sample,padding=\"max_length\"):\n",
    "    # add prefix to the input for t5\n",
    "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
    "\n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "    pass\n",
    "```\n",
    "1. It pads every input to absolute corpus max length. Would waste tons of memory and computation. The mean of dialogue is 149, meaning on average, 1k unnecessary tokens would be processed for each instance, and we have 14732 instances in training set. \n",
    "2. I use `flan-t5` which is the heir of LM adopted T5, that makes prepending `summarize:` to the input not necessary. \n",
    "\n",
    "## Training\n",
    "### Prepare for trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no truncation, since the max_length in the training set is only 1153. Should be fine.\n",
    "def preprocess(examples):\n",
    "    output = tokenizer(examples[\"dialogue\"])\n",
    "    output[\"labels\"] = tokenizer(examples[\"summary\"])[\"input_ids\"]\n",
    "    return output\n",
    "\n",
    "# tokenize the dataset\n",
    "tk_ds = ds.map(preprocess, batched=True).remove_columns(ds['train'].column_names)\n",
    "\n",
    "# load the evaluation metric\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# postprocessing necessary for rouge\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds\n",
    "    ]\n",
    "    decoded_labels = [\n",
    "        \"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels\n",
    "    ]\n",
    "\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    return result\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, padding=True pad_to_multiple_of=8)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=1,\n",
    "    bf16=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    predict_with_generate=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    hub_model_id=hub_model_id,\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tk_ds[\"train\"],\n",
    "    eval_dataset=tk_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fire up the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "total_flos = trainer.state.total_flos\n",
    "runtime = trainer.state.log_history[1]['train_runtime']\n",
    "utilization = total_flos / 1e12 / runtime # in tflops"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "`rouge-1: 47.8%` is in the same range with the source blog. However, to save time it's only trained for 1 epoch.\n",
    "\n",
    "### About TFLOPS\n",
    "- `model.parallelize()`\n",
    "  - `20.43` tflops.\n",
    "  - Peak memory: GPU1: 16.6G, GPU2: 14.9G\n",
    "- No `m.parallelize()`, vanilla huggingface trainer. \n",
    "  - `16.66` tflops.\n",
    "  - Peak memory: GPU1: 22.27, GPU2: 21.93G\n",
    "  - Higher GPU utilization, ~90%, slower training, more memory footprint. Why...?\n",
    "- `pad_to_multiple_of=64` -> `19.72` tflops\n",
    "  - Not ready to innovate on [dark magic](https://twitter.com/karpathy/status/1621578354024677377) yet LoL. \n",
    "- No `pad_to_multiple_of=8` -> `20.38` tflops\n",
    "  - No need to do this religiously. Make no difference with `pytorch` and this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "beadb2206a71d821b38106110d53f14ddca305a3c23cd1b533f09ecf1590880c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
